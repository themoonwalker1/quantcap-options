{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output file name\n",
    "output_file = \"consolidated_data.csv\"\n",
    "\n",
    "# Initialize a list to store merged DataFrames from all subfolders\n",
    "all_subfolder_data = []\n",
    "\n",
    "# Traverse through all subfolders in the current directory\n",
    "for root, dirs, files in os.walk(os.getcwd()):\n",
    "    infotable_df = None\n",
    "    coverpage_df = None\n",
    "    print(root)\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "\n",
    "        # Process INFOTABLE.tsv\n",
    "        if file == \"INFOTABLE.tsv\":\n",
    "            try:\n",
    "                print(f\"Processing: {file_path}\")\n",
    "                infotable_df = pd.read_csv(file_path, sep=\"\\t\", low_memory=False)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "        # Process COVERPAGE.tsv\n",
    "        if file == \"COVERPAGE.tsv\":\n",
    "            try:\n",
    "                print(f\"Processing: {file_path}\")\n",
    "                coverpage_df = pd.read_csv(file_path, sep=\"\\t\", low_memory=False)\n",
    "                # Keep only necessary columns\n",
    "                coverpage_df = coverpage_df[[\"ACCESSION_NUMBER\", \"FILINGMANAGER_NAME\", \"DATEREPORTED\"]]\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "    # Merge INFOTABLE and COVERPAGE on ACCESSION_NUMBER if both are present\n",
    "    if infotable_df is not None and coverpage_df is not None:\n",
    "        try:\n",
    "            merged_subfolder_data = pd.merge(infotable_df, coverpage_df, on=\"ACCESSION_NUMBER\", how=\"inner\")\n",
    "            all_subfolder_data.append(merged_subfolder_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging files in {root}: {e}\")\n",
    "\n",
    "# Combine all merged subfolder DataFrames into a single DataFrame\n",
    "if all_subfolder_data:\n",
    "    consolidated_data = pd.concat(all_subfolder_data, ignore_index=True)\n",
    "    consolidated_data.to_csv(output_file, index=False)\n",
    "    print(f\"Consolidation complete. File saved as {output_file}\")\n",
    "else:\n",
    "    print(\"No data to process.\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Function to check and print the fields of a CSV file\n",
    "def check_csv_fields(file_path):\n",
    "    \"\"\"\n",
    "    Load a CSV file and print its fields (column names) along with metadata.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load CSV into a DataFrame without loading all data into memory\n",
    "        df = pd.read_csv(file_path, nrows=0)\n",
    "\n",
    "        # Print column names\n",
    "        print(\"Field (Column) Names:\")\n",
    "        for column in df.columns:\n",
    "            print(f\"- {column}\")\n",
    "\n",
    "        # Print metadata\n",
    "        print(f\"\\nNumber of fields: {len(df.columns)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV file: {e}\")\n",
    "\n",
    "# Example usage\n",
    "# Replace 'your_file.csv' with the path to your CSV file\n",
    "check_csv_fields('consolidated_data_2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def delete_fields_from_csv(input_file, output_file, fields_to_delete):\n",
    "    \"\"\"\n",
    "    Deletes specified fields (columns) from a CSV file and saves the updated data to a new file.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the input CSV file.\n",
    "        output_file (str): Path to save the updated CSV file.\n",
    "        fields_to_delete (list): List of column names to delete from the CSV.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(input_file)\n",
    "\n",
    "        # Check if the columns to delete exist in the DataFrame\n",
    "        missing_fields = [field for field in fields_to_delete if field not in df.columns]\n",
    "        if missing_fields:\n",
    "            print(f\"Warning: The following fields were not found in the CSV and will be ignored: {missing_fields}\")\n",
    "\n",
    "        # Drop the specified fields\n",
    "        updated_df = df.drop(columns=fields_to_delete, errors='ignore')\n",
    "\n",
    "        # Save the updated DataFrame to a new CSV file\n",
    "        updated_df.to_csv(output_file, index=False)\n",
    "        print(f\"Updated CSV saved to '{output_file}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing the file: {e}\")\n",
    "\n",
    "# Example usage\n",
    "input_file = 'consolidated_data_2.csv'  # Replace with your input file path\n",
    "output_file = 'final_13f.csv'  # Replace with your desired output file path\n",
    "fields_to_delete = ['ACCESSION_NUMBER']  # Replace with the fields/columns you want to delete\n",
    "\n",
    "delete_fields_from_csv(input_file, output_file, fields_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class YourClass:\n",
    "    input_path = \"final_13f.csv\"\n",
    "\n",
    "    def CUSIP_to_ticker(self, output_csv_path=\"institutional_data.csv\"):\n",
    "        \"\"\"\n",
    "        Reads institutional data, maps CUSIP codes to tickers using a reference CSV,\n",
    "        removes invalid dates, and writes the resulting DataFrame to a CSV file.\n",
    "\n",
    "        Args:\n",
    "            output_csv_path (str): Path to save the resulting CSV file.\n",
    "\n",
    "        Returns:\n",
    "            str: Path to the generated CSV file.\n",
    "        \"\"\"\n",
    "        # Reference CSV for CUSIP-to-Ticker mapping\n",
    "        df_ref = pd.read_csv(\"cusip_tickers.csv\", dtype=str)\n",
    "\n",
    "        # Check if required columns exist\n",
    "        if not {\"CUSIP\", \"Ticker\"}.issubset(df_ref.columns):\n",
    "            raise ValueError(\"Reference CSV must have 'CUSIP' and 'Ticker' columns.\")\n",
    "\n",
    "        # Create a mapping dictionary\n",
    "        cusip_to_ticker = df_ref.set_index(\"CUSIP\")[\"Ticker\"].to_dict()\n",
    "\n",
    "        original_csv = self.input_path  # Path to the input file\n",
    "        chunksize = 10**5  # Process data in chunks of 100,000 rows\n",
    "\n",
    "        # Open the output file in write mode\n",
    "        with pd.read_csv(original_csv, dtype=str, chunksize=chunksize) as reader, open(output_csv_path, \"w\") as output_file:\n",
    "            for chunk_index, chunk in enumerate(reader):\n",
    "                print(f\"Processing chunk {chunk_index + 1}...\")\n",
    "\n",
    "                # Validate required columns\n",
    "                if \"CUSIP\" not in chunk.columns or \"DATEREPORTED\" not in chunk.columns:\n",
    "                    raise ValueError(\"Input CSV is missing 'CUSIP' or 'DATEREPORTED' columns.\")\n",
    "\n",
    "                # Map CUSIP to Ticker\n",
    "                chunk[\"CUSIP\"] = chunk[\"CUSIP\"].map(cusip_to_ticker).fillna(chunk[\"CUSIP\"])\n",
    "\n",
    "                # Rename 'CUSIP' column to 'Ticker'\n",
    "                chunk.rename(columns={\"CUSIP\": \"Ticker\", \"DATEREPORTED\": \"date\"}, inplace=True)\n",
    "\n",
    "                # Convert 'date' column to datetime and drop invalid dates\n",
    "                chunk[\"date\"] = pd.to_datetime(chunk[\"date\"], errors=\"coerce\")\n",
    "                chunk.dropna(subset=[\"date\"], inplace=True)  # Remove rows with invalid dates\n",
    "\n",
    "                # Debugging: Print the first few rows of the current chunk\n",
    "                print(chunk.head())\n",
    "\n",
    "                # Write the processed chunk to the output CSV\n",
    "                mode = \"w\" if chunk_index == 0 else \"a\"  # Write header only for the first chunk\n",
    "                header = chunk_index == 0\n",
    "                chunk.to_csv(output_file, index=False, mode=mode, header=header)\n",
    "\n",
    "        print(f\"Consolidated data has been saved to {output_csv_path}\")\n",
    "        return output_csv_path\n",
    "\n",
    "\n",
    "# Create an instance of YourClass and call the method\n",
    "analyzer = YourClass()\n",
    "output_csv_path = analyzer.CUSIP_to_ticker()\n",
    "print(f\"Output CSV generated at: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old code\n",
    "\n",
    "\"\"\"\n",
    "df_ref = pd.read_csv(\"cusip_tickers.csv\", dtype=str)\n",
    "cusip_to_ticker = df_ref.set_index(\"CUSIP\")[\"Ticker\"].to_dict()\n",
    "\n",
    "# Check columns exist\n",
    "if not {\"CUSIP\", \"Ticker\"}.issubset(df_ref.columns):\n",
    "    raise ValueError(\"Reference CSV must have 'CUSIP' and 'Ticker' columns.\")\n",
    "    \n",
    "# Convert to dict\n",
    "   \n",
    "def process_dataframe(df, cusip_to_ticker):\n",
    "    \"\"\"\n",
    "    Maps CUSIP codes to tickers within a DataFrame and renames the column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The original DataFrame with a 'CUSIP' column.\n",
    "        cusip_to_ticker (dict): Dictionary mapping CUSIP codes to tickers.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated DataFrame with 'CUSIP' replaced by 'Ticker'.\n",
    "    \"\"\"\n",
    "    # Check if \"CUSIP\" exists in the DataFrame\n",
    "    if \"CUSIP\" not in df.columns:\n",
    "        raise ValueError(\"The DataFrame is missing a 'CUSIP' column.\")\n",
    "\n",
    "    # Map CUSIP to Ticker and handle missing mappings\n",
    "    df[\"CUSIP\"] = df[\"CUSIP\"].map(cusip_to_ticker).fillna(df[\"CUSIP\"])\n",
    "\n",
    "    # Rename \"CUSIP\" column to \"Ticker\"\n",
    "    df.rename(columns={\"CUSIP\": \"Ticker\"}, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Process the DataFrame\n",
    "processed_df = process_dataframe(df, cusip_to_ticker)\n",
    "\n",
    "print(processed_df)\n",
    "      \n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/yfv_whfd2pv776k5pk93tvt80000gn/T/ipykernel_4738/3946070976.py:30: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  info_table_df = pd.read_csv(info_table_path, sep=\"\\t\")\n",
      "/var/folders/26/yfv_whfd2pv776k5pk93tvt80000gn/T/ipykernel_4738/3946070976.py:30: DtypeWarning: Columns (5,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  info_table_df = pd.read_csv(info_table_path, sep=\"\\t\")\n",
      "/var/folders/26/yfv_whfd2pv776k5pk93tvt80000gn/T/ipykernel_4738/3946070976.py:30: DtypeWarning: Columns (5,9,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  info_table_df = pd.read_csv(info_table_path, sep=\"\\t\")\n",
      "/var/folders/26/yfv_whfd2pv776k5pk93tvt80000gn/T/ipykernel_4738/3946070976.py:30: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  info_table_df = pd.read_csv(info_table_path, sep=\"\\t\")\n",
      "/var/folders/26/yfv_whfd2pv776k5pk93tvt80000gn/T/ipykernel_4738/3946070976.py:30: DtypeWarning: Columns (5,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  info_table_df = pd.read_csv(info_table_path, sep=\"\\t\")\n",
      "/var/folders/26/yfv_whfd2pv776k5pk93tvt80000gn/T/ipykernel_4738/3946070976.py:30: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  info_table_df = pd.read_csv(info_table_path, sep=\"\\t\")\n",
      "/var/folders/26/yfv_whfd2pv776k5pk93tvt80000gn/T/ipykernel_4738/3946070976.py:30: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  info_table_df = pd.read_csv(info_table_path, sep=\"\\t\")\n",
      "/var/folders/26/yfv_whfd2pv776k5pk93tvt80000gn/T/ipykernel_4738/3946070976.py:30: DtypeWarning: Columns (5,9,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  info_table_df = pd.read_csv(info_table_path, sep=\"\\t\")\n",
      "/var/folders/26/yfv_whfd2pv776k5pk93tvt80000gn/T/ipykernel_4738/3946070976.py:30: DtypeWarning: Columns (5,9,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  info_table_df = pd.read_csv(info_table_path, sep=\"\\t\")\n",
      "/var/folders/26/yfv_whfd2pv776k5pk93tvt80000gn/T/ipykernel_4738/3946070976.py:30: DtypeWarning: Columns (9,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  info_table_df = pd.read_csv(info_table_path, sep=\"\\t\")\n",
      "/var/folders/26/yfv_whfd2pv776k5pk93tvt80000gn/T/ipykernel_4738/3946070976.py:30: DtypeWarning: Columns (9,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  info_table_df = pd.read_csv(info_table_path, sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered GOOGL holdings data saved to /Users/lockin/Downloads/googl_holdings_aggregated.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sec_cik_mapper import StockMapper\n",
    "\n",
    "mapper = StockMapper()\n",
    "# Define the parent directory to process\n",
    "def process_googl_holdings(parent_directory):\n",
    "    result_data = []  # To store filtered and aggregated data\n",
    "\n",
    "    # Define GOOGL CUSIP\n",
    "    googl_cusip1 = \"02079K305\"\n",
    "    googl_cusip2 = \"02079K107\"\n",
    "\n",
    "    # Walk through all subfolders\n",
    "    for root, dirs, files in os.walk(parent_directory):\n",
    "        info_table_path = None\n",
    "        submission_path = None\n",
    "\n",
    "        # Identify required files in the current directory\n",
    "        for file in files:\n",
    "            if file.lower() == \"infotable.tsv\":\n",
    "                info_table_path = os.path.join(root, file)\n",
    "            elif file.lower() == \"submission.tsv\":\n",
    "                submission_path = os.path.join(root, file)\n",
    "\n",
    "        # If both files are found, process them\n",
    "        if info_table_path and submission_path:\n",
    "            # Load INFO TABLE\n",
    "            try:\n",
    "                info_table_df = pd.read_csv(info_table_path, sep=\"\\t\")\n",
    "                # Filter for GOOGL holdings\n",
    "                googl_holdings1 = info_table_df[info_table_df[\"CUSIP\"] == googl_cusip1]\n",
    "                googl_holdings2 = info_table_df[info_table_df[\"CUSIP\"] == googl_cusip2]\n",
    "                googl_holdings = pd.concat([googl_holdings1, googl_holdings2], ignore_index=True)\n",
    "                # print(info_table_df)\n",
    "                if googl_holdings.empty:\n",
    "                    continue  # Skip if no GOOGL holdings found\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading INFO TABLE at {info_table_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Load SUBMISSION\n",
    "            try:\n",
    "                submission_df = pd.read_csv(submission_path, sep=\"\\t\")\n",
    "                # print(submission_df)\n",
    "                # Join INFO TABLE with SUBMISSION on accession number\n",
    "                if \"ACCESSION_NUMBER\" in info_table_df.columns and \"ACCESSION_NUMBER\" in submission_df.columns:\n",
    "                    googl_holdings = googl_holdings.merge(\n",
    "                        submission_df, on=\"ACCESSION_NUMBER\", how=\"inner\"\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading SUBMISSION at {submission_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Combine GOOGL holdings with submission metadata\n",
    "            try:\n",
    "                for _, row in googl_holdings.iterrows():\n",
    "                    combined_data = {\n",
    "                        \"issuer_name\": row.get(\"NAMEOFISSUER\"),\n",
    "                        \"cusip\": row.get(\"CUSIP\"),\n",
    "                        \"value\": row.get(\"VALUE\"),\n",
    "                        \"shares\": row.get(\"SSHPRNAMT\"),\n",
    "                        \"share_type\": row.get(\"SSHPRNAMTTYPE\"),\n",
    "                        \"put_call\": row.get(\"PUTCALL\", None),\n",
    "                        \"submission_cik\": row.get(\"CIK\"),\n",
    "                        \"submission_ticker\": mapper.cik_to_tickers.get(str(row.get(\"CIK\")).zfill(10), None),\n",
    "                        \"submission_form_type\": row.get(\"SUBMISSIONTYPE\"),\n",
    "                        \"submission_period\": row.get(\"PERIODOFREPORT\"),\n",
    "                        \"submission_filing_date\": row.get(\"FILING_DATE\"),\n",
    "                    }\n",
    "                    result_data.append(combined_data)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing data from {info_table_path} and {submission_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Create final DataFrame\n",
    "    result_df = pd.DataFrame(result_data)\n",
    "\n",
    "    # Save the filtered and aggregated data to a CSV file\n",
    "    output_path = os.path.join(parent_directory, \"googl_holdings_aggregated.csv\")\n",
    "    result_df.to_csv(output_path, index=False)\n",
    "    print(f\"Filtered GOOGL holdings data saved to {output_path}\")\n",
    "\n",
    "# Specify the parent directory (replace this with the actual path)\n",
    "parent_directory = \"/Users/lockin/Downloads/\"\n",
    "process_googl_holdings(parent_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AIR'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mapper.cik_to_tickers\n",
    "mapper = StockMapper()\n",
    "cik_to_ticker = mapper.cik_to_tickers  \n",
    "cik_to_ticker.get(str(1750).zfill(10))# Dictionary mapping CIK to tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis saved to ./cik_holdings_analysis.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/yfv_whfd2pv776k5pk93tvt80000gn/T/ipykernel_7642/1413951334.py:24: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  grouped = grouped.fillna(method='ffill', axis=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "def analyze_cik_holdings(file_path, output_path):\n",
    "    # Load the data\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert submission_period to datetime for quarter-based grouping\n",
    "    df['submission_period'] = pd.to_datetime(df['submission_period'], format=\"%d-%b-%Y\")\n",
    "\n",
    "    # Extract year-quarter for grouping\n",
    "    df['year_quarter'] = df['submission_period'].dt.to_period('Q')\n",
    "\n",
    "    # Aggregate shares by CIK and year-quarter\n",
    "    grouped = df.groupby(['submission_cik', 'year_quarter'])['shares'].sum().unstack(fill_value=0)\n",
    "\n",
    "    # Create a DataFrame with percent changes for each quarter\n",
    "    percent_changes = grouped.pct_change(axis=1) * 100\n",
    "\n",
    "    # Fill missing percent changes with 0\n",
    "    percent_changes = percent_changes.fillna(0)\n",
    "\n",
    "    # Forward-fill missing values in the original data for accurate future changes\n",
    "    grouped = grouped.fillna(method='ffill', axis=1)\n",
    "\n",
    "    # Combine the original shares data with percent changes\n",
    "    combined_df = pd.concat([grouped, percent_changes.add_suffix('_percent_change')], axis=1)\n",
    "\n",
    "    # Save the result to a new CSV\n",
    "    combined_df.reset_index().to_csv(output_path, index=False)\n",
    "    print(f\"Analysis saved to {output_path}\")\n",
    "\n",
    "# File paths\n",
    "input_file_path = \"./googl_holdings_aggregated1.csv\"\n",
    "output_file_path = \"./cik_holdings_analysis.csv\"\n",
    "\n",
    "# Run the analysis\n",
    "analyze_cik_holdings(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
