{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/themoonwalker1/quantcap-options/blob/main/options_data_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "o7Z9N3kV2llC"
      },
      "outputs": [],
      "source": [
        "# Quant Cap Options Trading 23/24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DCPSey5G2p6Q"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import httpx\n",
        "import csv\n",
        "import io\n",
        "import yfinance as yf\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from httpx import HTTPStatusError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "UQ7MkZuo3eN3"
      },
      "outputs": [],
      "source": [
        "class GatherData():\n",
        "    \n",
        "    def get_historical_data(self, symbols: list[str], start_date: str, end_date: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "       \n",
        "        Pulls EOD Options, EOD Greeks, and Open Interest data from ThetaData API and returns it as a DataFrame\n",
        "         \n",
        "        Retrieves data pertaining to the given parameters (params) from the\n",
        "        historical option and Greeks EOD and Open Interest endpoints. The data\n",
        "        includes all possible expiration dates and strike prices within the\n",
        "        specified range. Reference: https://http-docs.thetadata.us/\n",
        "\n",
        "\n",
        "        Args:\n",
        "            symbols: A list of strings representing the tickers to fetch historical option data.\n",
        "            start_date: A string representing the start date for historical option data\n",
        "            end_date: A string representing the end date for the historical option data\n",
        "\n",
        "        Returns:\n",
        "            merged_df: A combined Pandas DataFrame of historical option EOD, historical Greeks EOD, and Open Interest data\n",
        "\n",
        "\n",
        "        Raises:\n",
        "            HTTPError: If the API request fails.\n",
        "            ValueError: If the response data is not in the expected format.\n",
        "        \"\"\"\n",
        "       \n",
        "        # BASE_URL = \"http://127.0.0.1:25510/v2\"\n",
        "        BASE_URL = \"http://127.0.0.1:25512/v2\"\n",
        "\n",
        "        tot_data = []\n",
        "        oi_data = []\n",
        "\n",
        "        start_date_parsed = datetime.strptime(start_date, \"%Y%m%d\")\n",
        "        end_date_parsed = datetime.strptime(end_date, \"%Y%m%d\")\n",
        "\n",
        "        header_eod = None\n",
        "        header_oi = None\n",
        "        for symbol in symbols:\n",
        "            current_date = start_date_parsed\n",
        "            while current_date <= end_date_parsed:              \n",
        "                params = {\n",
        "                    \"root\": symbol,\n",
        "                    \"start_date\": current_date.strftime(\"%Y%m%d\"),\n",
        "                    \"end_date\": current_date.strftime(\"%Y%m%d\"),\n",
        "                    # \"exp\": exp_date.strftime(\"%Y%m%d\"),\n",
        "                    \"exp\": 0,\n",
        "                    \"use_csv\": 'true'\n",
        "                }\n",
        "\n",
        "                urleod = BASE_URL + '/bulk_hist/option/eod_greeks'\n",
        "\n",
        "                header = None\n",
        "                while urleod is not None:\n",
        "                    try:\n",
        "                        response = httpx.get(urleod, params=params)\n",
        "                        response.raise_for_status()\n",
        "                        csv_data = csv.reader(response.text.split(\"\\n\"))\n",
        "                        if header is None:\n",
        "                            header = next(csv_data)\n",
        "                            if header_eod is None:\n",
        "                                header_eod = header\n",
        "                                expiration_idx_eod = header_eod.index('expiration')\n",
        "                                tot_data.append(header_eod)\n",
        "               \n",
        "                        for line in csv_data:\n",
        "                            if line:\n",
        "                                if True:\n",
        "                                    expiration_date = datetime.strptime(line[expiration_idx_eod], '%Y%m%d')\n",
        "                                    date_diff = (expiration_date - current_date).days\n",
        "                                    if 0 <= date_diff <= 30:\n",
        "                                        tot_data.append(line)\n",
        "                               \n",
        "                        if 'Next-Page' in response.headers and response.headers['Next-Page'] != \"null\":\n",
        "                            urleod = response.headers['Next-Page']\n",
        "                        else:\n",
        "                            urleod = None\n",
        "                    except HTTPStatusError as e:\n",
        "                        if e.response.status_code == 472:\n",
        "                            print(f\"No results for {symbol} on {current_date.strftime('%Y-%m-%d')} for EOD Greeks, skipping.\")\n",
        "                            break\n",
        "                        else:\n",
        "                            raise\n",
        "\n",
        "                urloi = BASE_URL + '/bulk_hist/option/open_interest'\n",
        "\n",
        "                header = None\n",
        "                while urloi is not None:\n",
        "                    try:\n",
        "                        response = httpx.get(urloi, params=params)\n",
        "                        response.raise_for_status()\n",
        "                        csv_data = csv.reader(response.text.split(\"\\n\"))\n",
        "                        if header is None:\n",
        "                            header = next(csv_data)\n",
        "                            if header_oi is None:\n",
        "                                header_oi = header\n",
        "                                expiration_idx_oi = header_oi.index('expiration')\n",
        "                                oi_data.append(header_oi)\n",
        "                        for line in csv_data:\n",
        "                            if line:\n",
        "                                if True:\n",
        "                                    expiration_date = datetime.strptime(line[expiration_idx_oi], '%Y%m%d')\n",
        "                                    date_diff = (expiration_date - current_date).days\n",
        "                                    if 0 <= date_diff <= 30:\n",
        "                                        oi_data.append(line)\n",
        "                                       \n",
        "                        if 'Next-Page' in response.headers and response.headers['Next-Page'] != \"null\":\n",
        "                            urloi = response.headers['Next-Page']\n",
        "                        else:\n",
        "                            urloi = None\n",
        "                    except HTTPStatusError as e:\n",
        "                        if e.response.status_code == 472:\n",
        "                            print(f\"No results for {symbol} on {current_date.strftime('%Y-%m-%d')} for Open Interest, skipping.\")\n",
        "                            break\n",
        "                        else:\n",
        "                            raise  \n",
        "                current_date += timedelta(days=1)\n",
        "\n",
        "        def process(data):\n",
        "            df = pd.DataFrame(data)\n",
        "            df.columns = df.iloc[0]\n",
        "            df = df.drop(0).reset_index(drop=True)\n",
        "            df['expiration'] = pd.to_datetime(df['expiration'], format='%Y%m%d', errors='coerce')\n",
        "            if df['expiration'].isnull().any():\n",
        "                print(\"Warning: Some 'expiration' values could not be parsed and were set to NaT.\")\n",
        "            df['date'] = pd.to_datetime(df['date'], format='%Y%m%d', errors='coerce')\n",
        "            if df['date'].isnull().any():\n",
        "                print(\"Warning: Some 'date' values could not be parsed and were set to NaT.\")\n",
        "            return df\n",
        "        df_eod = process(tot_data)\n",
        "        df_oi = process(oi_data)\n",
        "        df_merged = pd.merge(df_eod, df_oi, on=['root', 'expiration', 'strike', 'right', 'date'], how='left')\n",
        "\n",
        "        return df_merged\n",
        "\n",
        "\n",
        "\n",
        "    def get_sentiment_data(self, symbols: list[str], start_date: str, end_date: str, interval: str = \"1d\") -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Fetch sentiment data from StockGeist API and return it as a DataFrame.\n",
        "\n",
        "        This function retrieves sentiment data for a specified asset class and\n",
        "        location from the StockGeist API. The data includes both message and\n",
        "        article sentiment metrics for a given symbol over a specified date range.\n",
        "        Reference: https://docs.stockgeist.ai/\n",
        "\n",
        "        Args:\n",
        "            symbols: A list of strings representing the tickers to fetch sentiment data for.\n",
        "                eg: [\"AAPL\", \"MSFT\", \"GOOGL\"]\n",
        "            start_date: A string representing the start date for the sentiment data.\n",
        "            end_date: A string representing the end date for the sentiment data.\n",
        "            interval: A string representing the interval for the sentiment data.\n",
        "        Returns:\n",
        "            DataFrame: A pandas DataFrame containing the sentiment data.\n",
        "\n",
        "        Raises:\n",
        "            HTTPError: If the API request fails.\n",
        "            ValueError: If the response data is not in the expected format.\n",
        "        \"\"\"\n",
        "\n",
        "        # constants\n",
        "        EM_WEIGHT = 1.5 # weight of emotional sentiment\n",
        "        NEM_WEIGHT = 1.0 # weight of non-emotional sentiment\n",
        "\n",
        "        MESSAGE_WEIGHT = 0.3\n",
        "        ARTICLE_WEIGHT = 0.7\n",
        "\n",
        "        # API Key\n",
        "        STOCKGEIST_API_KEY = 'oElOiXrwLtDY2flizauAIrrpivXU0bUQ'\n",
        "        headers = {\"token\": STOCKGEIST_API_KEY}\n",
        "\n",
        "        # API endpoint\n",
        "        base_url = \"https://api.stockgeist.ai\"\n",
        "        asset_class = \"stock\"  # or \"crypto\"\n",
        "        location = \"us\"  # or \"global\"\n",
        "\n",
        "        # Messages sentiment data\n",
        "        messages_url = f\"{base_url}/{asset_class}/{location}/hist/message-metrics\"\n",
        "        messages_params = {\n",
        "            \"symbols\": ','.join(symbols),\n",
        "            \"start\": start_date,\n",
        "            \"end\": end_date,\n",
        "            \"timeframe\": interval\n",
        "        }\n",
        "        \n",
        "        # response = requests.get(messages_url, headers=headers, params=messages_params)\n",
        "        # if response.status_code != 200:\n",
        "        #     raise requests.HTTPError(f\"{response.status_code}: {response.text}\")\n",
        "        \n",
        "        # messages_data = response.json().get(\"data\", {})\n",
        "        messages_data = json.load(open(\"messages.json\")).get(\"data\", {})\n",
        "\n",
        "        messages_date_rows = []\n",
        "        for symbol in symbols: # same stocks in symbols\n",
        "            stock_data = messages_data.get(symbol, [])\n",
        "            for day_data in stock_data:\n",
        "                date = datetime.fromisoformat(day_data.get(\"timestamp\")).date() # date only\n",
        "                pos_em_count = day_data.get(\"pos_em_count\", 0)\n",
        "                pos_nem_count = day_data.get(\"pos_nem_count\", 0)\n",
        "                neu_em_count = day_data.get(\"neu_em_count\", 0)\n",
        "                neu_nem_count = day_data.get(\"neu_nem_count\", 0)\n",
        "                neg_em_count = day_data.get(\"neg_em_count\", 0)\n",
        "                neg_nem_count = day_data.get(\"neg_nem_count\", 0)\n",
        "                em_total_count = day_data.get(\"em_total_count\", 0)\n",
        "                nem_total_count = day_data.get(\"nem_total_count\", 0)\n",
        "                pos_total_count = day_data.get(\"pos_total_count\", 0)\n",
        "                neu_total_count = day_data.get(\"neu_total_count\", 0)\n",
        "                neg_total_count = day_data.get(\"neg_total_count\", 0)\n",
        "                total_count = day_data.get(\"total_count\", 0)\n",
        "\n",
        "                # calculate message sentiment using custom formula\n",
        "                #normalized: 0 to 1\n",
        "                # message_sentiment = EM_WEIGHT * (pos_em_count + neg_em_count) / em_total_count + NEM_WEIGHT * (pos_nem_count + neg_nem_count) / nem_total_count\n",
        "                # messages_date_rows.append([symbol, date, message_sentiment])\n",
        "                messages_date_rows.append([symbol, date, pos_em_count, neg_em_count, em_total_count, pos_nem_count, neu_em_count, neu_nem_count, neg_nem_count, nem_total_count, pos_total_count, neu_total_count, neg_total_count, total_count])\n",
        "\n",
        "        messages_df = pd.DataFrame(messages_date_rows, columns=[\"symbol\", \"date\", \"pos_em_count\", \"neg_em_count\", \"em_total_count\", \"pos_nem_count\", \"neu_em_count\", \"neu_nem_count\", \"neg_nem_count\", \"nem_total_count\", \"pos_total_count\", \"neu_total_count\", \"neg_total_count\", \"total_count\"])\n",
        "\n",
        "        # Articles sentiment data\n",
        "        articles_url = f\"{base_url}/{asset_class}/{location}/hist/article-metrics\"\n",
        "        articles_params = {\n",
        "            \"symbols\": ','.join(symbols),\n",
        "            \"start\": start_date,\n",
        "            \"end\": end_date,\n",
        "            \"timeframe\": interval,\n",
        "            \"max_symbol_articles\": 200,\n",
        "            \"sort_by\": \"timestamp\"\n",
        "        }\n",
        "        # response = requests.get(articles_url, headers=headers, params=articles_params)\n",
        "        # if response.status_code != 200:\n",
        "        #     raise requests.HTTPError(f\"{response.status_code}: {response.text}\")\n",
        "        \n",
        "        # articles_data = response.json().get(\"data\", {})\n",
        "\n",
        "        articles_data = json.load(open(\"articles.json\")).get(\"data\", {})\n",
        "\n",
        "        articles_date_rows = []\n",
        "        for symbol in symbols:\n",
        "            stock_data = articles_data.get(symbol, [])  \n",
        "            for day_data in stock_data:\n",
        "                date = datetime.fromisoformat(day_data.get(\"timestamp\")).date() # date only\n",
        "                mentions = day_data.get(\"mentions\", 0)\n",
        "                title_sentiment = day_data.get(\"title_sentiment\", \"neutral\")\n",
        "                sentiment_map = {\"neutral\": 0, \"positive\": 1, \"negative\": -1} # # neutral, positive, negative\n",
        "                title_sentiment = sentiment_map.get(title_sentiment, 0)\n",
        "                # ignore title, summary, original_url, img_url, sentiment_spans\n",
        "\n",
        "                # calculate article sentiment using custom formula\n",
        "                article_sentiment = title_sentiment * mentions  \n",
        "\n",
        "                articles_date_rows.append([symbol, date, article_sentiment, mentions])\n",
        "\n",
        "        # symbol/date pair should be unique\n",
        "        articles_df = pd.DataFrame(articles_date_rows, columns=[\"symbol\", \"date\", \"article_sentiment\", \"article_count\"]).groupby(['symbol', 'date'], as_index=False)[['article_sentiment', 'article_count']].sum()\n",
        "\n",
        "\n",
        "        # combine messages and articles dataframes\n",
        "        sentiment_df = pd.merge(messages_df, articles_df, on=[\"symbol\", \"date\"], how=\"outer\")\n",
        "        sentiment_df.fillna(0, inplace=True)\n",
        "        # sentiment_df.set_index([\"symbol\", \"date\"], inplace=True)\n",
        "\n",
        "        # calculate weighted sentiment\n",
        "        # sentiment_df[\"weighted_sentiment\"] = MESSAGE_WEIGHT * sentiment_df[\"message_sentiment\"] + ARTICLE_WEIGHT * sentiment_df[\"article_sentiment\"]\n",
        "\n",
        "        return sentiment_df\n",
        "\n",
        "\n",
        "    def get_fundamental_data(self, ticker: str, total_years: int):\n",
        "        \"\"\" \n",
        "        Pulls historical stock data from Tiingo using its API and \n",
        "        preprocesses it by getting the percentage stock increase per \n",
        "        week from the raw EOD close data\n",
        "\n",
        "        Args: \n",
        "            ticker: A string with the specified stock ticker\n",
        "            total_years: How many years back to get the data from the current date. \n",
        "            Must be less than 30. \n",
        "        Returns: \n",
        "            Pandas Dataframe with two columns: the date and the percentage stock increase a week out into the future. \n",
        "            The date is an integer in the form YYYY-MM-DD, and the percentage stock increase is a float. \n",
        "        Raises: \n",
        "            ValueError if the total_years inputted are greater than 30. \n",
        "        \"\"\"\n",
        "        if total_years > 30:\n",
        "            raise ValueError(\"Total years are greater than 30\")\n",
        "\n",
        "        TIINGO_API_KEY = '95b8e93dadad1cceda98479bc2420f9a0bb5556a'\n",
        "        base_url = \"https://api.tiingo.com/tiingo/daily\"\n",
        "        curr_date = date.today()\n",
        "        start_date = date(curr_date) + relativedelta(years=total_years)\n",
        "        str_end_date = str(curr_date)\n",
        "        str_start_date = str(start_date)\n",
        "        ticker = ticker.lower()\n",
        "        url = f\"{base_url}/{ticker}/prices?startDate={str_start_date}&endDate={str_end_date}&token={TIINGO_API_KEY}\"\n",
        "        headers = {\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "        requestResponse = requests.get(\"https://api.tiingo.com/tiingo/daily/aapl/prices?startDate=2023-12-02&token=95b8e93dadad1cceda98479bc2420f9a0bb5556a\", headers=headers)\n",
        "        requestResponse = requestResponse.json()\n",
        "        size = len(requestResponse)\n",
        "        to_return = {\"date\": [None] * size, \"pct_change\": np.ndarray((size))}\n",
        "        for i in range(len(requestResponse) - 7):\n",
        "            entry = requestResponse[i]\n",
        "            date = entry[\"date\"]\n",
        "            end_date = datetime.fromisoformat(date[:-1] + '+00:00') + relativedelta(days=7)\n",
        "            end_date = end_date.isoformat()[:-6] + '.000Z'\n",
        "            match_close = [a['close'] for a in requestResponse if a['date']== end_date]\n",
        "            if len(match_close) != 0:\n",
        "                day = date[:10]\n",
        "                close_1 = entry['close']\n",
        "                close_2 = match_close[0]\n",
        "                pct_change = (close_2 - close_1) / close_1 * 100\n",
        "                to_return[\"date\"][i] = day\n",
        "                to_return[\"pct_change\"][i] = pct_change\n",
        "        indices_of_none_dates = [x for x in range(len(to_return[\"date\"])) if to_return[\"date\"][x] is not None]\n",
        "        to_return[\"date\"] = [to_return[\"date\"][x] for x in indices_of_none_dates]\n",
        "        to_return[\"pct_change\"] = [to_return[\"pct_change\"][x] for x in indices_of_none_dates]\n",
        "        to_return = pd.DataFrame.from_dict(to_return)\n",
        "        return to_return\n",
        "\n",
        "\n",
        "    def get_liquidity_data(self):\n",
        "        \"\"\" Pulls data from NASDAQ and returns a dataframe \"\"\"\n",
        "        pass\n",
        "\n",
        "    def get_earnings_data(self, symbols: list[str], start_date: str, end_date: str):\n",
        "        \"\"\"\n",
        "        Fetch earnings data from Yahoo Calendar and return it as a DataFrame.\n",
        "\n",
        "        This function retrieves earnings data for a specified list of tickers\n",
        "        from the Yahoo Calendar API. The data includes earnings dates and other\n",
        "        relevant financial information for the given tickers over a specified date range.\n",
        "        Reference:  https://pypi.org/project/yfinance/\n",
        "                    https://ranaroussi.github.io/yfinance/index.html    \n",
        "\n",
        "        Args:\n",
        "            symbols: A list of strings representing the tickers to fetch earnings data for.\n",
        "                eg: [\"AAPL\", \"MSFT\", \"GOOGL\"]\n",
        "            start_date: A string representing the start date for the earnings data.\n",
        "            end_date: A string representing the end date for the earnings data.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame: A pandas DataFrame containing the earnings data.\n",
        "\n",
        "        Raises:\n",
        "            HTTPError: If the API request fails.\n",
        "            ValueError: If the response data is not in the expected format.\n",
        "        \"\"\"\n",
        "        \n",
        "        earnings_df = pd.DataFrame(columns=[\"symbol\", \"earnings_date\", \"estimated_eps\", \"reported_eps\"])\n",
        "        for symbol in symbols:\n",
        "            ticker = yf.Ticker(symbol)\n",
        "            earning_dates_df = ticker.earnings_dates\n",
        "            earning_dates_df.drop(columns=[\"Surprise(%)\"], inplace=True)\n",
        "            earning_dates_df.reset_index(drop=False, inplace=True)\n",
        "            earning_dates_df.columns= [\"earnings_date\", \"estimated_eps\", \"reported_eps\"]\n",
        "            earning_dates_df[\"symbol\"] = symbol\n",
        "            earning_dates_df[\"earnings_date\"] = earning_dates_df[\"earnings_date\"].dt.date\n",
        "            earnings_df = pd.concat([earnings_df, earning_dates_df], ignore_index=True)\n",
        "            earnings_df.dropna(axis=1, how='all', inplace=True)\n",
        "        return earnings_df\n",
        "\n",
        "    def get_institutional_data(self):\n",
        "        \"\"\" Pulls data from 13F-Form Dataset and returns a dataframe\"\"\"\n",
        "        pass\n",
        "\n",
        "    def json_to_dtf(self):\n",
        "        \"\"\" Parses through response and constructs and returns a dataframe\"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No results for AAPL on 2024-01-01 for EOD Greeks, skipping.\n",
            "No results for AAPL on 2024-01-01 for Open Interest, skipping.\n",
            "0     root expiration  strike right ms_of_day_x    open    high     low  \\\n",
            "0     AAPL 2024-01-05  225000     C    35403554  0.0100  0.0100  0.0100   \n",
            "1     AAPL 2024-01-05  225000     P           0  0.0000  0.0000  0.0000   \n",
            "2     AAPL 2024-01-05  227500     C           0  0.0000  0.0000  0.0000   \n",
            "3     AAPL 2024-01-05  227500     P           0  0.0000  0.0000  0.0000   \n",
            "4     AAPL 2024-01-05  230000     P           0  0.0000  0.0000  0.0000   \n",
            "...    ...        ...     ...   ...         ...     ...     ...     ...   \n",
            "2037  AAPL 2024-01-05  220000     C           0  0.0000  0.0000  0.0000   \n",
            "2038  AAPL 2024-02-02  220000     P           0  0.0000  0.0000  0.0000   \n",
            "2039  AAPL 2024-02-02  220000     C    57325904  0.0300  0.0400  0.0200   \n",
            "2040  AAPL 2024-01-05  222500     P           0  0.0000  0.0000  0.0000   \n",
            "2041  AAPL 2024-01-05  222500     C           0  0.0000  0.0000  0.0000   \n",
            "\n",
            "0      close volume  ...    ultima        d1        d2 implied_vol  iv_error  \\\n",
            "0     0.0100      5  ...    9.7091   -2.7756   -2.8439      0.7531    0.0269   \n",
            "1     0.0000      0  ...    0.0000    0.0000    0.0000      0.0000  100.0000   \n",
            "2     0.0000      0  ...    0.4524   -4.4533   -4.4986      0.5000  100.0000   \n",
            "3     0.0000      0  ...    0.0000    0.0000    0.0000      0.0000  100.0000   \n",
            "4     0.0000      0  ...    0.0000    0.0000    0.0000      0.0000  100.0000   \n",
            "...      ...    ...  ...       ...       ...       ...         ...       ...   \n",
            "2037  0.0000      0  ...    0.0000  -19.1458  -19.1559      0.5000    0.0000   \n",
            "2038  0.0000      0  ...    0.0000    0.0000    0.0000      0.0000  100.0000   \n",
            "2039  0.0400     17  ...  100.0000   -2.3838   -2.4622      0.2832   -0.0093   \n",
            "2040  0.0000      0  ...    0.0000    0.0000    0.0000      0.0000  100.0000   \n",
            "2041  0.0000      0  ...    0.0000  -20.2606  -20.2707      0.5000    0.0000   \n",
            "\n",
            "0    ms_of_day2 underlying_price       date ms_of_day_y open_interest  \n",
            "0      35403554         185.6400 2024-01-02    23412000           379  \n",
            "1             0         185.6400 2024-01-02    23410000             0  \n",
            "2             0         185.6400 2024-01-02    23413000            82  \n",
            "3             0         185.6400 2024-01-02    23411000             0  \n",
            "4             0         185.6400 2024-01-02    23414000             0  \n",
            "...         ...              ...        ...         ...           ...  \n",
            "2037          0         181.1800 2024-01-05    23401000          1202  \n",
            "2038          0         181.1800 2024-01-05    23418000             0  \n",
            "2039   57325904         181.1800 2024-01-05    23404000           544  \n",
            "2040          0         181.1800 2024-01-05    23407000             0  \n",
            "2041          0         181.1800 2024-01-05    23404000            29  \n",
            "\n",
            "[2042 rows x 44 columns]\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "my_data = GatherData()\n",
        "\n",
        "\n",
        "historical_df = my_data.get_historical_data([\"AAPL\"], \"20240101\", \"20240105\")\n",
        "print(historical_df)\n",
        "\n",
        "\n",
        "# Saving data to CSV\n",
        "historical_df.to_csv('historical_options_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  symbol        date  pos_em_count  neg_em_count  em_total_count  \\\n",
            "0   AAPL  2024-01-01            36            13              62   \n",
            "1  GOOGL  2024-01-01             6             3              11   \n",
            "\n",
            "   pos_nem_count  neu_em_count  neu_nem_count  neg_nem_count  nem_total_count  \\\n",
            "0             49            13             11             27               87   \n",
            "1             14             2              1              3               18   \n",
            "\n",
            "   pos_total_count  neu_total_count  neg_total_count  total_count  \\\n",
            "0               85               24               40          149   \n",
            "1               20                3                6           29   \n",
            "\n",
            "   article_sentiment  article_count  \n",
            "0                -14             42  \n",
            "1                 -3              3  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/h4/v901gzsn6dqb4xb5ffzltz3r0000gp/T/ipykernel_6942/3257886038.py:331: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  earnings_df = pd.concat([earnings_df, earning_dates_df], ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  symbol earnings_date  estimated_eps  reported_eps\n",
            "0   AAPL    2025-10-28            NaN           NaN\n",
            "1   AAPL    2025-07-29            NaN           NaN\n",
            "2   AAPL    2025-04-29            NaN           NaN\n",
            "3   AAPL    2025-01-29           2.36           NaN\n",
            "4   AAPL    2024-10-30           1.60          1.64\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "\n",
        "analyzer = GatherData()\n",
        "sentiment_df = analyzer.get_sentiment_data([\"AAPL\", \"GOOGL\"], \"2024-01-01\", \"2024-01-02\", \"1d\")\n",
        "print(sentiment_df.head())\n",
        "earnings_df = analyzer.get_earnings_data([\"AAPL\", \"GOOGL\"], \"2024-01-01\", \"2024-01-02\")\n",
        "print(earnings_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4q6TYiPP3kJF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class ProcessData():\n",
        "    def join_all_datasets(self, arr, methods):\n",
        "        \"\"\"\n",
        "        joins all of the datasets in an array of dataframes using the\n",
        "         specified methods in the string array of methods.\n",
        "         Methods include full, left, right, inner\"\"\"\n",
        "        pass\n",
        "\n",
        "    \"\"\" handles missing NaN values using an approach passed in as a string variable named approach for dataset d. Approaches allowed include one-hot encoding, dropping all rows with NaN, etc.\"\"\"\n",
        "\n",
        "    def handle_missing_values(self, approach, dataset):\n",
        "        pass\n",
        "    \"\"\" Normalize features of dataframe x using sklearn methods using an approach passed in as a string variable approach\"\"\"\n",
        "\n",
        "    def normalize_features(self, x, approach):\n",
        "        pass\n",
        "\n",
        "    \"\"\" Select target features of x, where the features are given in an array feature list\"\"\"\n",
        "\n",
        "    def select_features(self, dataset, feature_list):\n",
        "        pass\n",
        "    \"\"\" Splits data into training, validation, and testing \"\"\"\n",
        "\n",
        "    def split_data(self, dataset, column, test_size, val_size):\n",
        "        pass\n",
        "    \"\"\" Deletes rows of data containing outliers above a certain threshold from dataset \"\"\"\n",
        "\n",
        "    def handle_outliers(self, dataset, threshold):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1myFYcBO2kng"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
