{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/themoonwalker1/quantcap-options/blob/main/options_data_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7Z9N3kV2llC"
      },
      "outputs": [],
      "source": [
        "# Quant Cap Options Trading 23/24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "DCPSey5G2p6Q"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import httpx\n",
        "import csv\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "UQ7MkZuo3eN3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ms_of_day  ms_of_day2   open   high   low  close  volume  count  bid_size  \\\n",
            "0   62488618    57597806   9.35  10.50  9.13  10.30    2215    432         1   \n",
            "1   62261982    57572197  10.20  10.95  9.50   9.96     640    198         7   \n",
            "2   62559535    57580493   8.68   8.95  6.95   8.35    4426    682        11   \n",
            "3   62498880    57574993   8.55   8.95  7.85   8.20    1211    423        10   \n",
            "\n",
            "   bid_exchange   bid  bid_condition  ask_size  ask_exchange    ask  \\\n",
            "0             1  9.40             50        32             7  10.50   \n",
            "1            47  9.90             50        97             4  10.05   \n",
            "2            76  8.30             50        70            76   8.40   \n",
            "3            60  8.15             50        90            76   8.30   \n",
            "\n",
            "   ask_condition      date  \n",
            "0             50  20241107  \n",
            "1             50  20241108  \n",
            "2             50  20241111  \n",
            "3             50  20241112  \n"
          ]
        }
      ],
      "source": [
        "class GatherData():\n",
        "    \n",
        "    def get_historical_data(self, params):\n",
        "        \"\"\" Pulls EOD Options, EOD Greeks, and Open Interest data from ThetaData API and returns it as a DataFrame\n",
        "         \n",
        "        Retrieves data pertaining to the given parameters (params) from the \n",
        "        historical option EOD, historical Greeks EOD, and Open Interest endpoints.\n",
        "\n",
        "        Args:\n",
        "            params: A dictionary of query parameters\n",
        "\n",
        "        Returns:\n",
        "            df: A combined Pandas DataFrame of historical option EOD, historical Greeks EOD, and Open Interest data\n",
        "\n",
        "        Raises:\n",
        "            HTTPError: If the API request fails.\n",
        "            ValueError: If the response data is not in the expected format.\n",
        "        \"\"\"\n",
        "        \n",
        "        BASE_URL = \"http://127.0.0.1:25510/v2\"\n",
        "\n",
        "        params = {\n",
        "            \"root\": \"AAPL\",\n",
        "            \"exp\": \"20250117\",\n",
        "            \"right\": \"C\",\n",
        "            \"strike\": \"225000\",\n",
        "            \"start_date\": \"20241107\",\n",
        "            \"end_date\": \"20241112\",\n",
        "            \"use_csv\": \"true\",\n",
        "            \"ivl\": \"60000\",\n",
        "        }\n",
        "\n",
        "        # Historical Data Options EOD\n",
        "\n",
        "        urleod = BASE_URL + '/hist/option/eod'\n",
        "\n",
        "        all_data = []\n",
        "        while urleod is not None:\n",
        "            with httpx.stream(\"GET\", urleod, params=params) as response:\n",
        "                response.raise_for_status()\n",
        "\n",
        "                for line in response.iter_lines():\n",
        "                    if line:\n",
        "                        all_data.append(line)\n",
        "                    #   data = json.loads(line)\n",
        "                    #   all_data.append(data)\n",
        "\n",
        "            if 'Next-Page' in response.headers and response.headers['Next-Page'] != \"null\":\n",
        "                urleod = response.headers['Next-Page']\n",
        "\n",
        "            else:\n",
        "                urleod = None\n",
        "\n",
        "        csv_data = \"\\n\".join(all_data)\n",
        "         \n",
        "        df_eod = pd.read_csv(io.StringIO(csv_data))\n",
        "\n",
        "\n",
        "\n",
        "        # Historical Data Options Greeks\n",
        "        \n",
        "        urlgreek = BASE_URL + '/bulk_hist/option/eod_greeks'\n",
        "        csv_greeks = ''\n",
        "\n",
        "        while url is not None:\n",
        "          response = httpx.get(urlgreek, params=params)\n",
        "          response.raise_for_status()\n",
        "          if 'Next-Page' in response.headers and response.headers['Next=Page'] != \"null\":\n",
        "            url = resp.headers['Next-Page']\n",
        "          else:\n",
        "            url = None\n",
        "          csv_greeks = csv_greeks + \"/n\" + response.text\n",
        "\n",
        "\n",
        "\n",
        "        # Historical Data Options Open Interest\n",
        "\n",
        "        urloi = BASE_URL + 'bulk_hist/option/open_interest'\n",
        "        csv_oi = ''\n",
        "\n",
        "        while url is not None:\n",
        "          response = httpx.get(urloi, params=params)\n",
        "          response.raise_for_status()\n",
        "          if 'Next-Page' in response.headers and response.headers['Next=Page'] != \"null\":\n",
        "            url = resp.headers['Next-Page']\n",
        "          else:\n",
        "            url = None\n",
        "          csv_oi = csv_oi + \"/n\" + response.text\n",
        "\n",
        "\n",
        "        return df_eod\n",
        "        \n",
        "\n",
        "\n",
        "    def get_sentiment_data(self, symbols: list[str], start_date: str, end_date: str, interval: str = \"1d\") -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Fetch sentiment data from StockGeist API and return it as a DataFrame.\n",
        "\n",
        "        This function retrieves sentiment data for a specified asset class and\n",
        "        location from the StockGeist API. The data includes both message and\n",
        "        article sentiment metrics for a given symbol over a specified date range.\n",
        "\n",
        "        Args:\n",
        "            symbols: A list of strings representing the tickers to fetch sentiment data for.\n",
        "                eg: [\"AAPL\", \"MSFT\", \"GOOGL\"]\n",
        "            start_date: A string representing the start date for the sentiment data.\n",
        "            end_date: A string representing the end date for the sentiment data.\n",
        "            interval: A string representing the interval for the sentiment data.\n",
        "        Returns:\n",
        "            DataFrame: A pandas DataFrame containing the sentiment data.\n",
        "\n",
        "        Raises:\n",
        "            HTTPError: If the API request fails.\n",
        "            ValueError: If the response data is not in the expected format.\n",
        "        \"\"\"\n",
        "\n",
        "        # constants\n",
        "        EM_WEIGHT = 1.5 # weight of emotional sentiment\n",
        "        NEM_WEIGHT = 1.0 # weight of non-emotional sentiment\n",
        "\n",
        "        MESSAGE_WEIGHT = 0.3\n",
        "        ARTICLE_WEIGHT = 0.7\n",
        "\n",
        "        # API Key\n",
        "        STOCKGEIST_API_KEY = ''\n",
        "        headers = {\"token\": STOCKGEIST_API_KEY}\n",
        "\n",
        "        # API endpoint\n",
        "        base_url = \"https://api.stockgeist.ai\"\n",
        "        asset_class = \"stocks\"  # or \"crypto\"\n",
        "        location = \"us\"  # or \"global\"\n",
        "\n",
        "        # Messages sentiment data\n",
        "        messages_url = f\"{base_url}/{asset_class}/{location}/hist/message-metrics\"\n",
        "        messages_params = {\n",
        "            \"symbols\": ','.join(symbols),\n",
        "            \"start\": start_date,\n",
        "            \"end\": end_date,\n",
        "            \"timeframe\": interval\n",
        "        }\n",
        "        \n",
        "        response = requests.get(messages_url, headers=headers, params=messages_params)\n",
        "        if response.status_code != 200:\n",
        "            raise requests.HTTPError(f\"Failed to fetch messages sentiment data: {response.status_code}\")\n",
        "        \n",
        "        messages_data = response.json().get(\"data\", [])\n",
        "\n",
        "        messages_df = pd.DataFrame(columns=[\"symbol\", \"date\", \"message_sentiment\"])\n",
        "\n",
        "        for symbol in symbols: # same stocks in symbols\n",
        "            stock_data = messages_data.get(symbol, [])\n",
        "            row = [symbol]\n",
        "            for day_data in stock_data:\n",
        "                date = day_data.get(\"date\", datetime.now().strftime(\"%Y-%m-%d\"))\n",
        "                row.append(date)\n",
        "                pos_em_count = day_data.get(\"pos_em_count\", 0)\n",
        "                pos_nem_count = day_data.get(\"pos_nem_count\", 0)\n",
        "                neu_em_count = day_data.get(\"neu_em_count\", 0)\n",
        "                neu_nem_count = day_data.get(\"neu_nem_count\", 0)\n",
        "                neg_em_count = day_data.get(\"neg_em_count\", 0)\n",
        "                neg_nem_count = day_data.get(\"neg_nem_count\", 0)\n",
        "                em_total_count = day_data.get(\"em_total_count\", 0)\n",
        "                nem_total_count = day_data.get(\"nem_total_count\", 0)\n",
        "                pos_total_count = day_data.get(\"pos_total_count\", 0)\n",
        "                neu_total_count = day_data.get(\"neu_total_count\", 0)\n",
        "                neg_total_count = day_data.get(\"neg_total_count\", 0)\n",
        "                total_count = day_data.get(\"total_count\", 0)\n",
        "\n",
        "                # calculate message sentiment using custom formula\n",
        "                #normalized: 0 to 1\n",
        "                message_sentiment = EM_WEIGHT * (pos_em_count + neg_em_count) / em_total_count + NEM_WEIGHT * (pos_nem_count + neg_nem_count) / nem_total_count\n",
        "                row.append(message_sentiment)\n",
        "            messages_df = messages_df.append(row, ignore_index=True)\n",
        "\n",
        "        # Articles sentiment data\n",
        "        articles_url = f\"{base_url}/{asset_class}/{location}/hist/article-metrics\"\n",
        "        articles_params = {\n",
        "            \"symbols\": ','.join(symbols),\n",
        "            \"start\": start_date,\n",
        "            \"end\": end_date,\n",
        "            \"timeframe\": interval,\n",
        "            \"max_symbol_articles\": 200,\n",
        "            \"sort_by\": \"timestamp\"\n",
        "        }\n",
        "        response = requests.get(articles_url, headers=headers, params=articles_params)\n",
        "        if response.status_code != 200:\n",
        "            raise requests.HTTPError(f\"Failed to fetch articles sentiment data: {response.status_code}\")\n",
        "        \n",
        "        articles_data = response.json().get(\"data\", [])\n",
        "\n",
        "        articles_df = pd.DataFrame(columns=[\"symbol\", \"date\", \"article_sentiment\"])\n",
        "\n",
        "        \n",
        "        for symbol in symbols:\n",
        "            stock_data = articles_data.get(symbol, [])  \n",
        "            row = [symbol]\n",
        "            for day_data in stock_data:\n",
        "                date = day_data.get(\"timestamp\", datetime.now().strftime(\"%Y-%m-%d\"))\n",
        "                row.append(date)\n",
        "                mentions = day_data.get(\"mentions\", 0)\n",
        "                title_sentiment = day_data.get(\"title_sentiment\", \"neutral\")\n",
        "                sentiment_map = {\"neutral\": 0, \"positive\": 1, \"negative\": -1} # # neutral, positive, negative\n",
        "                title_sentiment = sentiment_map.get(title_sentiment, 0)\n",
        "                # ignore title, summary, original_url, img_url, sentiment_spans\n",
        "\n",
        "                # calculate article sentiment using custom formula\n",
        "\n",
        "                article_sentiment = title_sentiment * mentions  \n",
        "                row.append(article_sentiment)\n",
        "\n",
        "            # check if a row exist for articles of this date for this symbol\n",
        "            date = row[1]\n",
        "            existing_row = articles_df[(articles_df['symbol'] == symbol) & (articles_df['date'] == date)]\n",
        "            if existing_row.empty: \n",
        "                articles_df = articles_df.append(row, ignore_index=True)\n",
        "            else: \n",
        "                articles_df.loc[(articles_df['symbol'] == symbol) & (articles_df['date'] == date), 'article_sentiment'] += article_sentiment\n",
        "\n",
        "        # combine messages and articles dataframes\n",
        "        sentiment_df = pd.merge([messages_df, articles_df], on=[\"symbol\", \"date\"], how=\"outer\")\n",
        "        sentiment_df.set_index([\"symbol\", \"date\"], inplace=True)\n",
        "\n",
        "        # calculate weighted sentiment\n",
        "        sentiment_df[\"weighted_sentiment\"] = MESSAGE_WEIGHT * sentiment_df[\"message_sentiment\"] + ARTICLE_WEIGHT * sentiment_df[\"article_sentiment\"]\n",
        "\n",
        "        return sentiment_df\n",
        "\n",
        "\n",
        "    def get_fundamental_data(self):\n",
        "        \"\"\" Pulls data from Tingo and returns a dataframe\"\"\"\n",
        "        pass\n",
        "\n",
        "    \"\"\" Pulls data from NASDAQ and returns a dataframe \"\"\"\n",
        "\n",
        "    def get_liquidity_data(self):\n",
        "        pass\n",
        "    \"\"\" Pulls data from Yahoo Calendar and returns a dataframe\"\"\"\n",
        "\n",
        "    def get_earnings_data(self):\n",
        "        pass\n",
        "    \"\"\" Pulls data from 13F-Form Dataset and returns a dataframe\"\"\"\n",
        "\n",
        "    def get_institutional_data(self):\n",
        "        pass\n",
        "    \"\"\" Parses through response and constructs and returns a dataframe\"\"\"\n",
        "\n",
        "    def json_to_dtf(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4q6TYiPP3kJF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class ProcessData():\n",
        "    def join_all_datasets(self, arr, methods):\n",
        "        \"\"\"\n",
        "        joins all of the datasets in an array of dataframes using the\n",
        "         specified methods in the string array of methods.\n",
        "         Methods include full, left, right, inner\"\"\"\n",
        "        pass\n",
        "\n",
        "    \"\"\" handles missing NaN values using an approach passed in as a string variable named approach for dataset d. Approaches allowed include one-hot encoding, dropping all rows with NaN, etc.\"\"\"\n",
        "\n",
        "    def handle_missing_values(self, approach, dataset):\n",
        "        pass\n",
        "    \"\"\" Normalize features of dataframe x using sklearn methods using an approach passed in as a string variable approach\"\"\"\n",
        "\n",
        "    def normalize_features(self, x, approach):\n",
        "        pass\n",
        "\n",
        "    \"\"\" Select target features of x, where the features are given in an array feature list\"\"\"\n",
        "\n",
        "    def select_features(self, dataset, feature_list):\n",
        "        pass\n",
        "    \"\"\" Splits data into training, validation, and testing \"\"\"\n",
        "\n",
        "    def split_data(self, dataset, column, test_size, val_size):\n",
        "        pass\n",
        "    \"\"\" Deletes rows of data containing outliers above a certain threshold from dataset \"\"\"\n",
        "\n",
        "    def handle_outliers(self, dataset, threshold):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMabI7pu1Pka",
        "outputId": "1748518f-b748-4796-9909-a979a679ae4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "testing\n",
            "hi\n"
          ]
        }
      ],
      "source": [
        "print(\"testing\")\n",
        "print(\"hi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kjy8Asj6KPT"
      },
      "outputs": [],
      "source": [
        "def fetch_smalltable_rows(\n",
        "    table_handle: smalltable.Table,\n",
        "    keys: Sequence[bytes | str],\n",
        "    require_all_keys: bool = False,\n",
        ") -> Mapping[bytes, tuple[str, ...]]:\n",
        "    \"\"\"Fetches rows from a Smalltable.\n",
        "\n",
        "    Retrieves rows pertaining to the given keys from the Table instance\n",
        "    represented by table_handle.  String keys will be UTF-8 encoded.\n",
        "\n",
        "    Args:\n",
        "        table_handle: An open smalltable.Table instance.\n",
        "        keys: A sequence of strings representing the key of each table\n",
        "          row to fetch.  String keys will be UTF-8 encoded.\n",
        "        require_all_keys: If True only rows with values set for all keys will be\n",
        "          returned.\n",
        "\n",
        "    Returns:\n",
        "        A dict mapping keys to the corresponding table row data\n",
        "        fetched. Each row is represented as a tuple of strings. For\n",
        "        example:\n",
        "\n",
        "        {b'Serak': ('Rigel VII', 'Preparer'),\n",
        "         b'Zim': ('Irk', 'Invader'),\n",
        "         b'Lrrr': ('Omicron Persei 8', 'Emperor')}\n",
        "\n",
        "        Returned keys are always bytes.  If a key from the keys argument is\n",
        "        missing from the dictionary, then that row was not found in the\n",
        "        table (and require_all_keys must have been False).\n",
        "\n",
        "    Raises:\n",
        "        IOError: An error occurred accessing the smalltable.\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1myFYcBO2kng"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
