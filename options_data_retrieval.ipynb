{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/themoonwalker1/quantcap-options/blob/main/options_data_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o7Z9N3kV2llC"
      },
      "outputs": [],
      "source": [
        "# Quant Cap Options Trading 23/24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta, date\n",
        "import json\n",
        "import httpx\n",
        "import csv\n",
        "import zstandard as zstd\n",
        "import io\n",
        "import yfinance as yf\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from httpx import HTTPStatusError\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os.path as osp\n",
        "import pickle\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# my_data = GatherData()\n",
        "\n",
        "# df = my_data.get_historical_data(['GOOGL'], '2024-01-01','2025-01-01')\n",
        "# print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UQ7MkZuo3eN3"
      },
      "outputs": [],
      "source": [
        "class GatherData():\n",
        "    \n",
        "    def get_historical_data(self, symbols: list[str], start_date: str, end_date: str, save_to_csv: bool = True) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "       \n",
        "        Pulls EOD Options, EOD Greeks, and Open Interest data from ThetaData API and returns it as a DataFrame\n",
        "         \n",
        "        Retrieves data pertaining to the given parameters (params) from the\n",
        "        historical option and Greeks EOD and Open Interest endpoints. The data\n",
        "        includes all possible expiration dates and strike prices within the\n",
        "        specified range. Reference: https://http-docs.thetadata.us/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        Args:\n",
        "            symbols: A list of strings representing the tickers to fetch historical option data.\n",
        "            start_date: A string representing the start date for historical option data\n",
        "            end_date: A string representing the end date for the historical option data\n",
        "            save_to_csv: A boolean for whether to save the dataframe as a csv file\n",
        "\n",
        "\n",
        "        Returns:\n",
        "            merged_df: A combined Pandas DataFrame of historical option EOD, historical Greeks EOD, and Open Interest data\n",
        "\n",
        "\n",
        "        Raises:\n",
        "            HTTPError: If the API request fails.\n",
        "            ValueError: If the response data is not in the expected format.\n",
        "        \"\"\"\n",
        "        file_path = f'historical_data_{\"_\".join(symbols)}_{start_date}-{end_date}.csv.zst'\n",
        "        if Path('../Quantcap-Options/' + file_path).exists():\n",
        "            return self.zst_to_dataframe('../Quantcap-Options/' + file_path, delimiter=',')\n",
        "       \n",
        "        # BASE_URL = \"http://127.0.0.1:25510/v2\"\n",
        "        BASE_URL = \"http://127.0.0.1:25512/v2\"\n",
        "\n",
        "        tot_data = []\n",
        "        oi_data = []\n",
        "\n",
        "        start_date_parsed = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
        "        end_date_parsed = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
        "\n",
        "        header_eod = None\n",
        "        header_oi = None\n",
        "        for symbol in symbols:\n",
        "            current_date = start_date_parsed\n",
        "            while current_date <= end_date_parsed:              \n",
        "                params = {\n",
        "                    \"root\": symbol,\n",
        "                    \"start_date\": current_date.strftime(\"%Y%m%d\"),\n",
        "                    \"end_date\": current_date.strftime(\"%Y%m%d\"),\n",
        "                    \"exp\": 0,\n",
        "                    \"use_csv\": 'true'\n",
        "                }\n",
        "\n",
        "                urleod = BASE_URL + '/bulk_hist/option/eod_greeks'\n",
        "\n",
        "                header = None\n",
        "                while urleod is not None:\n",
        "                    try:\n",
        "                        response = httpx.get(urleod, params=params)\n",
        "                        response.raise_for_status()\n",
        "                        csv_data = csv.reader(response.text.split(\"\\n\"))\n",
        "                        if header is None:\n",
        "                            header = next(csv_data)\n",
        "                            if header_eod is None:\n",
        "                                header_eod = header\n",
        "                                expiration_idx_eod = header_eod.index('expiration')\n",
        "                                tot_data.append(header_eod)\n",
        "               \n",
        "                        for line in csv_data:\n",
        "                            if line:\n",
        "                                if True:\n",
        "                                    expiration_date = datetime.strptime(line[expiration_idx_eod], '%Y%m%d')\n",
        "                                    date_diff = (expiration_date - current_date).days\n",
        "                                    if 0 <= date_diff <= 30:\n",
        "                                        tot_data.append(line)\n",
        "                               \n",
        "                        if 'Next-Page' in response.headers and response.headers['Next-Page'] != \"null\":\n",
        "                            urleod = response.headers['Next-Page']\n",
        "                        else:\n",
        "                            urleod = None\n",
        "                    except HTTPStatusError as e:\n",
        "                        if e.response.status_code == 472:\n",
        "                            print(f\"No results for {symbol} on {current_date.strftime('%Y-%m-%d')} for EOD Greeks, skipping.\")\n",
        "                            break\n",
        "                        else:\n",
        "                            raise\n",
        "\n",
        "                urloi = BASE_URL + '/bulk_hist/option/open_interest'\n",
        "\n",
        "                header = None\n",
        "                while urloi is not None:\n",
        "                    try:\n",
        "                        response = httpx.get(urloi, params=params)\n",
        "                        response.raise_for_status()\n",
        "                        csv_data = csv.reader(response.text.split(\"\\n\"))\n",
        "                        if header is None:\n",
        "                            header = next(csv_data)\n",
        "                            if header_oi is None:\n",
        "                                header_oi = header\n",
        "                                expiration_idx_oi = header_oi.index('expiration')\n",
        "                                oi_data.append(header_oi)\n",
        "                        for line in csv_data:\n",
        "                            if line:\n",
        "                                if True:\n",
        "                                    expiration_date = datetime.strptime(line[expiration_idx_oi], '%Y%m%d')\n",
        "                                    date_diff = (expiration_date - current_date).days\n",
        "                                    if 0 <= date_diff <= 30:\n",
        "                                        oi_data.append(line)\n",
        "                                       \n",
        "                        if 'Next-Page' in response.headers and response.headers['Next-Page'] != \"null\":\n",
        "                            urloi = response.headers['Next-Page']\n",
        "                        else:\n",
        "                            urloi = None\n",
        "                    except HTTPStatusError as e:\n",
        "                        if e.response.status_code == 472:\n",
        "                            print(f\"No results for {symbol} on {current_date.strftime('%Y-%m-%d')} for Open Interest, skipping.\")\n",
        "                            break\n",
        "                        else:\n",
        "                            raise  \n",
        "                current_date += timedelta(days=1)\n",
        "\n",
        "        def process(data):\n",
        "            df = pd.DataFrame(data)\n",
        "            df.columns = df.iloc[0]\n",
        "            df = df.drop(0).reset_index(drop=True)\n",
        "            df['expiration'] = pd.to_datetime(df['expiration'], format='%Y%m%d', errors='coerce')\n",
        "            if df['expiration'].isnull().any():\n",
        "                print(\"Warning: Some 'expiration' values could not be parsed and were set to NaT.\")\n",
        "            df['date'] = pd.to_datetime(df['date'], format='%Y%m%d', errors='coerce')\n",
        "            if df['date'].isnull().any():\n",
        "                print(\"Warning: Some 'date' values could not be parsed and were set to NaT.\")\n",
        "            return df\n",
        "        df_eod = process(tot_data)\n",
        "        df_oi = process(oi_data)\n",
        "\n",
        "        df_merged = pd.merge(df_eod, df_oi, on=['root', 'expiration', 'strike', 'right', 'date'], how='left')\n",
        "\n",
        "        df_merged = df_merged[['date', 'root', 'expiration', 'strike', 'right', 'close', 'volume', 'count',\n",
        "                               'bid', 'bid_size', 'ask', 'ask_size', 'open_interest', 'delta', 'theta', 'vega', 'rho',\n",
        "                               'epsilon', 'lambda', 'gamma', 'd1', 'd2', 'implied_vol', 'iv_error', 'underlying_price']]\n",
        "\n",
        "        df_merged = df_merged.rename(columns={'root': 'ticker'})\n",
        "\n",
        "        if save_to_csv:\n",
        "            df_merged.to_csv(file_path, index=False)\n",
        "\n",
        "        return df_merged\n",
        "\n",
        "\n",
        "\n",
        "    def get_sentiment_data(self, symbols: list[str], start_date: str, end_date: str, interval: str = \"1d\") -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Fetch sentiment data from StockGeist API and return it as a DataFrame.\n",
        "\n",
        "        This function retrieves sentiment data for a specified asset class and\n",
        "        location from the StockGeist API. The data includes both message and\n",
        "        article sentiment metrics for a given symbol over a specified date range.\n",
        "        Reference: https://docs.stockgeist.ai/\n",
        "\n",
        "        Args:\n",
        "            symbols: A list of strings representing the tickers to fetch sentiment data for.\n",
        "                eg: [\"AAPL\", \"MSFT\", \"GOOGL\"]\n",
        "            start_date: A string representing the start date for the sentiment data.\n",
        "            end_date: A string representing the end date for the sentiment data.\n",
        "            interval: A string representing the interval for the sentiment data.\n",
        "        Returns:\n",
        "            DataFrame: A pandas DataFrame containing the sentiment data.\n",
        "\n",
        "        Raises:\n",
        "            HTTPError: If the API request fails.\n",
        "            ValueError: If the response data is not in the expected format.\n",
        "        \"\"\"\n",
        "\n",
        "        #files\n",
        "        messages_file_path = \"sentiment_data_messages.json\"\n",
        "        articles_file_path = \"sentiment_data_articles.json\"\n",
        "\n",
        "        # API Key\n",
        "        STOCKGEIST_API_KEY = \"b2s0InjGYj5JJN5SvLf2gWjiAorevDVd\" #'oElOiXrwLtDY2flizauAIrrpivXU0bUQ'\n",
        "        headers = {\"token\": STOCKGEIST_API_KEY}\n",
        "\n",
        "        # API endpoint\n",
        "        base_url = \"https://api.stockgeist.ai\"\n",
        "        asset_class = \"stock\"  # or \"crypto\"\n",
        "        location = \"us\"  # or \"global\"\n",
        "\n",
        "        # Messages sentiment data\n",
        "        messages_url = f\"{base_url}/{asset_class}/{location}/hist/message-metrics\"\n",
        "        messages_params = {\n",
        "            \"symbols\": ','.join(symbols),\n",
        "            \"start\": start_date,\n",
        "            \"end\": end_date,\n",
        "            \"timeframe\": interval\n",
        "        }\n",
        "        \n",
        "        messages_data = {}\n",
        "        if osp.isfile(messages_file_path):\n",
        "            with open(messages_file_path, 'r') as f:\n",
        "                messages_data = json.load(f)\n",
        "        else:\n",
        "            response = requests.get(messages_url, headers=headers, params=messages_params)\n",
        "            if response.status_code != 200:\n",
        "                raise requests.HTTPError(f\"{response.status_code}: {response.text}\")\n",
        "        \n",
        "            messages_data = response.json().get(\"data\", {})\n",
        "            with open(messages_file_path, 'w') as f:\n",
        "                json.dump(messages_data, f) \n",
        "    \n",
        "        messages_date_rows = []\n",
        "        for symbol in symbols: # same stocks in symbols\n",
        "            stock_data = messages_data.get(symbol, [])\n",
        "            for day_data in stock_data:\n",
        "                date = datetime.fromisoformat(day_data.get(\"timestamp\")).date() # date only\n",
        "                pos_em_count = day_data.get(\"pos_em_count\", 0)\n",
        "                pos_nem_count = day_data.get(\"pos_nem_count\", 0)\n",
        "                neu_em_count = day_data.get(\"neu_em_count\", 0)\n",
        "                neu_nem_count = day_data.get(\"neu_nem_count\", 0)\n",
        "                neg_em_count = day_data.get(\"neg_em_count\", 0)\n",
        "                neg_nem_count = day_data.get(\"neg_nem_count\", 0)\n",
        "                em_total_count = day_data.get(\"em_total_count\", 0)\n",
        "                nem_total_count = day_data.get(\"nem_total_count\", 0)\n",
        "                pos_total_count = day_data.get(\"pos_total_count\", 0)\n",
        "                neu_total_count = day_data.get(\"neu_total_count\", 0)\n",
        "                neg_total_count = day_data.get(\"neg_total_count\", 0)\n",
        "                total_count = day_data.get(\"total_count\", 0)\n",
        "\n",
        "                # calculate message sentiment using custom formula\n",
        "                #normalized: 0 to 1\n",
        "                # message_sentiment = EM_WEIGHT * (pos_em_count + neg_em_count) / em_total_count + NEM_WEIGHT * (pos_nem_count + neg_nem_count) / nem_total_count\n",
        "                # messages_date_rows.append([symbol, date, message_sentiment])\n",
        "                messages_date_rows.append([symbol, date, pos_em_count, neg_em_count, em_total_count, pos_nem_count, neu_em_count, neu_nem_count, neg_nem_count, nem_total_count, pos_total_count, neu_total_count, neg_total_count, total_count])\n",
        "\n",
        "        messages_df = pd.DataFrame(messages_date_rows, columns=[\"ticker\", \"date\", \"pos_em_count\", \"neg_em_count\", \"em_total_count\", \"pos_nem_count\", \"neu_em_count\", \"neu_nem_count\", \"neg_nem_count\", \"nem_total_count\", \"pos_total_count\", \"neu_total_count\", \"neg_total_count\", \"total_count\"])\n",
        "\n",
        "        # Articles sentiment data\n",
        "        articles_url = f\"{base_url}/{asset_class}/{location}/hist/article-metrics\"\n",
        "        articles_params = {\n",
        "            \"symbols\": ','.join(symbols),\n",
        "            \"start\": start_date,\n",
        "            \"end\": end_date,\n",
        "            \"timeframe\": interval,\n",
        "            \"max_symbol_articles\": 200,\n",
        "            \"sort_by\": \"timestamp\"\n",
        "        }\n",
        "        articles_data = {}\n",
        "        if osp.isfile(articles_file_path):\n",
        "            with open(articles_file_path, 'r') as f:\n",
        "                articles_data = json.load(f)\n",
        "        else:\n",
        "            response = requests.get(articles_url, headers=headers, params=articles_params)\n",
        "            if response.status_code != 200: \n",
        "                raise requests.HTTPError(f\"{response.status_code}: {response.text}\")\n",
        "            articles_data = response.json().get(\"data\", {})\n",
        "            with open(articles_file_path, 'w') as f:\n",
        "                json.dump(articles_data, f)\n",
        "\n",
        "        articles_date_rows = []\n",
        "        for symbol in symbols:\n",
        "            stock_data = articles_data.get(symbol, [])  \n",
        "            for day_data in stock_data:\n",
        "                date = datetime.fromisoformat(day_data.get(\"timestamp\")).date() # date only\n",
        "                mentions = day_data.get(\"mentions\", 0)\n",
        "                title_sentiment = day_data.get(\"title_sentiment\", \"neutral\")\n",
        "                sentiment_map = {\"neutral\": 0, \"positive\": 1, \"negative\": -1} # # neutral, positive, negative\n",
        "                title_sentiment = sentiment_map.get(title_sentiment, 0)\n",
        "                # ignore title, summary, original_url, img_url, sentiment_spans\n",
        "\n",
        "                # calculate article sentiment using custom formula\n",
        "                article_sentiment = title_sentiment * mentions  \n",
        "\n",
        "                articles_date_rows.append([symbol, date, article_sentiment, mentions])\n",
        "\n",
        "        # symbol/date pair should be unique\n",
        "        articles_df = pd.DataFrame(articles_date_rows, columns=[\"ticker\", \"date\", \"article_sentiment\", \"article_count\"]).groupby(['ticker', 'date'], as_index=False)[['article_sentiment', 'article_count']].sum()\n",
        "\n",
        "\n",
        "        # combine messages and articles dataframes\n",
        "        sentiment_df = pd.merge(messages_df, articles_df, on=[\"ticker\", \"date\"], how=\"outer\")\n",
        "        sentiment_df.fillna(0, inplace=True)\n",
        "        # sentiment_df.set_index([\"symbol\", \"date\"], inplace=True)\n",
        "\n",
        "        # calculate weighted sentiment\n",
        "        # sentiment_df[\"weighted_sentiment\"] = MESSAGE_WEIGHT * sentiment_df[\"message_sentiment\"] + ARTICLE_WEIGHT * sentiment_df[\"article_sentiment\"]\n",
        "\n",
        "        return sentiment_df\n",
        "\n",
        "\n",
        "    def get_fundamental_data(self, ticker: str, start_date: str, end_date: str):\n",
        "        \"\"\" \n",
        "        Pulls historical stock data from Tiingo using its API and \n",
        "        preprocesses it by getting the percentage stock increase per \n",
        "        week from the raw EOD close data\n",
        "\n",
        "        Args: \n",
        "            ticker: A string with the specified stock ticker\n",
        "            start_date: Starting date of the data returned\n",
        "            end_date: End date of the data returned\n",
        "        Returns: \n",
        "            Pandas Dataframe with two columns: the date and the percentage stock increase a week out into the future. \n",
        "            The date is an integer in the form YYYY-MM-DD, and the percentage stock increase is a float. \n",
        "        Raises: \n",
        "            ValueError if the total_years inputted are greater than 30. \n",
        "        \"\"\"\n",
        "        file_name = \"fundamental\"\n",
        "        cur_path = osp.dirname(osp.abspath(\"__file__\"))\n",
        "        path_to_file = osp.join(cur_path, '{}.pickle'.format(file_name))\n",
        "        to_return = None\n",
        "        if osp.isfile(path_to_file):\n",
        "            to_return = pickle.load(open(path_to_file, 'rb'))[0]\n",
        "        else:\n",
        "            TIINGO_API_KEY = '95b8e93dadad1cceda98479bc2420f9a0bb5556a'\n",
        "            base_url = \"https://api.tiingo.com/tiingo/daily\"\n",
        "            url = f\"{base_url}/{ticker.lower()}/prices?startDate={start_date}&endDate={end_date}&token={TIINGO_API_KEY}\"\n",
        "            headers = {\n",
        "                'Content-Type': 'application/json'\n",
        "            }\n",
        "            requestResponse = requests.get(url, headers=headers)\n",
        "            requestResponse = requestResponse.json()\n",
        "            # print(requestResponse)\n",
        "            # size = len(requestResponse)\n",
        "            # to_return = {\"date\": [None] * size, \"pct_change\": np.ndarray((size))}\n",
        "            # for i in range(7, len(requestResponse)):\n",
        "            #     entry = requestResponse[i]\n",
        "            #     temp_date = entry[\"date\"]\n",
        "            #     end_date = datetime.fromisoformat(temp_date[:-1] + '+00:00') - relativedelta(days=7)\n",
        "            #     end_date = end_date.isoformat()[:-6] + '.000Z'\n",
        "            #     match_close = [a['close'] for a in requestResponse if a['date']== end_date]\n",
        "            #     if len(match_close) != 0:\n",
        "            #         day = temp_date[:10]\n",
        "            #         close_1 = entry['close']\n",
        "            #         close_2 = match_close[0]\n",
        "            #         pct_change = (close_1 - close_2) / close_2 * 100\n",
        "            #         to_return[\"date\"][i] = day\n",
        "            #         to_return[\"pct_change\"][i] = pct_change\n",
        "            #     indices_of_none_dates = [x for x in range(len(to_return[\"date\"])) if to_return[\"date\"][x] is not None]\n",
        "            #     to_return[\"date\"] = [to_return[\"date\"][x] for x in indices_of_none_dates]\n",
        "            #     to_return[\"pct_change\"] = [to_return[\"pct_change\"][x] for x in indices_of_none_dates]\n",
        "            to_return = pd.json_normalize(requestResponse)\n",
        "            to_return[\"ticker\"] = ticker\n",
        "            to_return[\"date\"] = pd.to_datetime(to_return[\"date\"]).dt.date\n",
        "            with open(path_to_file, 'wb') as cachedfile:\n",
        "                pickle.dump((to_return,), cachedfile)\n",
        "        return to_return\n",
        "\n",
        "\n",
        "    def zst_to_dataframe(self, file_path, delimiter=','):\n",
        "        \"\"\"\n",
        "        Converts a Zstandard-compressed file (.zst) to a pandas DataFrame.\n",
        "\n",
        "        Args:\n",
        "            file_path (str): Path to the .zst file.\n",
        "            delimiter (str): Delimiter used in the CSV file (default is ',').\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: The resulting pandas DataFrame.\n",
        "        \"\"\"\n",
        "        with open(file_path, 'rb') as f:\n",
        "                # Create a Zstandard decompressor\n",
        "                dctx = zstd.ZstdDecompressor()\n",
        "\n",
        "                # Decompress the file using a streaming reader\n",
        "                with dctx.stream_reader(f) as reader:\n",
        "                    # Wrap the decompressed data into a text wrapper\n",
        "                    decompressed_data = io.TextIOWrapper(reader, encoding='utf-8')\n",
        "                    \n",
        "                    # Read the decompressed data into a DataFrame\n",
        "                    df = pd.read_csv(decompressed_data, delimiter=delimiter)\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def shrink_dataframe(df, fields_to_keep):\n",
        "        \"\"\"\n",
        "        Reduces a DataFrame to only the specified fields (columns).\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): The original DataFrame.\n",
        "            fields_to_keep (list): List of column names to retain.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: A reduced DataFrame containing only the specified fields.\n",
        "        \"\"\"\n",
        "        # Check if all fields to keep are in the DataFrame\n",
        "        missing_fields = [field for field in fields_to_keep if field not in df.columns]\n",
        "        if missing_fields:\n",
        "            raise ValueError(f\"The following fields are not in the DataFrame: {missing_fields}\")\n",
        "\n",
        "        # Keep only the specified columns\n",
        "        reduced_df = df[fields_to_keep]\n",
        "        return reduced_df\n",
        "    \n",
        "    def get_liquidity_data(self, symbols: list[str], start_date: str, end_date: str):\n",
        "        \"\"\"\n",
        "        Pulls data from NASDAQ, filters it by symbols and date range, and returns a DataFrame.\n",
        "\n",
        "        Args:\n",
        "            symbols: A list of strings representing the tickers to fetch liquidity data for.\n",
        "            start_date: A string representing the start date (inclusive) in \"YYYY-MM-DD\" format.\n",
        "            end_date: A string representing the end date (inclusive) in \"YYYY-MM-DD\" format.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame: A pandas DataFrame containing the filtered liquidity data.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If required fields are missing in the dataset.\n",
        "        \"\"\"\n",
        "        # File path for the data (replace with your actual .zst file path)\n",
        "        file_path = 'xnas-itch-20241209.mbp-1.csv.zst'\n",
        "        df = self.zst_to_dataframe(file_path)  # Load the .zst file into a DataFrame\n",
        "\n",
        "\n",
        "        # Fields to keep\n",
        "        fields_to_keep = [\n",
        "            \"ts_event\", \"action\", \"size\", \"price\", \"bid_px_00\", \"ask_px_00\", \n",
        "            \"bid_sz_00\", \"ask_sz_00\", \"bid_ct_00\", \"ask_ct_00\", \"symbol\"\n",
        "        ]\n",
        "        # Shrink the DataFrame to only the fields we need\n",
        "        df = df[fields_to_keep]\n",
        "\n",
        "        # Convert 'ts_event' to datetime and create a 'date' column\n",
        "        df[\"date\"] = pd.to_datetime(df[\"ts_event\"]).dt.date\n",
        "\n",
        "        # Filter by date range\n",
        "        start_date = pd.to_datetime(start_date).date()\n",
        "        end_date = pd.to_datetime(end_date).date()\n",
        "        df = df[(df[\"date\"] >= start_date) & (df[\"date\"] <= end_date)]\n",
        "\n",
        "        # Filter by symbols\n",
        "        df = df[df[\"symbol\"].isin(symbols)]\n",
        "        \n",
        "        df = df[df[\"action\"].isin([\"A\", \"T\"])]\n",
        "\n",
        "        # Rename 'symbol' to 'ticker'\n",
        "        df.rename(columns={\"symbol\": \"ticker\"}, inplace=True)\n",
        "\n",
        "        # Drop the 'ts_event' column\n",
        "        df.drop(columns=[\"ts_event\"], inplace=True)\n",
        "\n",
        "        return df\n",
        "\n",
        "        # Original file was >7GB thus most processing was done beforehand by code in 13F_parser.ipynb. institutional_data.csv is already cleaned.\n",
        "\n",
        "    def get_institutional_data(self, input_path, symbols: list[str], start_date: str, end_date: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Reads institutional data from a CSV file, filters it by symbols and date range,\n",
        "        and returns a DataFrame.\n",
        "\n",
        "        Args:\n",
        "            input_path (str): Path to the input CSV file.\n",
        "            symbols (list[str]): List of tickers to filter the data for.\n",
        "            start_date (str): Start date (inclusive) in \"YYYY-MM-DD\" format.\n",
        "            end_date (str): End date (inclusive) in \"YYYY-MM-DD\" format.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: A DataFrame containing the filtered institutional data.\n",
        "        \"\"\"\n",
        "        # Read the input CSV file\n",
        "        df = pd.read_csv(input_path, dtype=str)\n",
        "\n",
        "        # Ensure the 'date' column is properly converted to datetime\n",
        "        if \"date\" not in df.columns:\n",
        "            raise ValueError(\"The input CSV is missing a 'date' column.\")\n",
        "        \n",
        "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "        df.dropna(subset=[\"date\"], inplace=True)  # Remove rows with invalid dates\n",
        "\n",
        "        # Filter by date range\n",
        "        start_date = pd.to_datetime(start_date)\n",
        "        end_date = pd.to_datetime(end_date)\n",
        "        \n",
        "        df = df[(df[\"date\"] >= start_date) & (df[\"date\"] <= end_date)]\n",
        "\n",
        "        # Filter data by symbols\n",
        "        if \"Ticker\" not in df.columns:\n",
        "            raise ValueError(\"The input CSV is missing a 'Ticker' column.\")\n",
        "        \n",
        "        df = df[df[\"FILINGMANAGER_NAME\"].isin(symbols)]\n",
        "\n",
        "        df.columns = df.columns.str.lower()\n",
        "\n",
        "        # Return the filtered DataFrame\n",
        "        return df\n",
        "\n",
        "    def get_earnings_data(self, symbols: list[str], start_date: str, end_date: str):\n",
        "        \"\"\"\n",
        "        Fetch earnings data from Yahoo Calendar and return it as a DataFrame.\n",
        "\n",
        "        This function retrieves earnings data for a specified list of tickers\n",
        "        from the Yahoo Calendar API. The data includes earnings dates and other\n",
        "        relevant financial information for the given tickers over a specified date range.\n",
        "        Reference:  https://pypi.org/project/yfinance/\n",
        "                    https://ranaroussi.github.io/yfinance/index.html    \n",
        "\n",
        "        Args:\n",
        "            symbols: A list of strings representing the tickers to fetch earnings data for.\n",
        "                eg: [\"AAPL\", \"MSFT\", \"GOOGL\"]\n",
        "            start_date: A string representing the start date for the earnings data.\n",
        "            end_date: A string representing the end date for the earnings data.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame: A pandas DataFrame containing the earnings data.\n",
        "\n",
        "        Raises:\n",
        "            HTTPError: If the API request fails.\n",
        "            ValueError: If the response data is not in the expected format.\n",
        "        \"\"\"\n",
        "        \n",
        "        earnings_df = pd.DataFrame(columns=[\"ticker\", \"earnings_date\", \"estimated_eps\", \"reported_eps\"])\n",
        "        for symbol in symbols:\n",
        "            ticker = yf.Ticker(symbol)\n",
        "            earning_dates_df = ticker.earnings_dates\n",
        "            earning_dates_df.drop(columns=[\"Surprise(%)\"], inplace=True)\n",
        "            earning_dates_df.reset_index(drop=False, inplace=True)\n",
        "            earning_dates_df.columns= [\"date\", \"estimated_eps\", \"reported_eps\"]\n",
        "            earning_dates_df[\"ticker\"] = symbol\n",
        "            earning_dates_df[\"date\"] = earning_dates_df[\"date\"].dt.date\n",
        "            earning_dates_df.dropna(axis=1, how='all', inplace=True)\n",
        "            earnings_df.dropna(axis=1, how='all', inplace=True)\n",
        "            earnings_df = pd.concat([earnings_df, earning_dates_df], ignore_index=True)\n",
        "        earnings_df = earnings_df[ (pd.to_datetime(start_date) <= pd.to_datetime(earnings_df[\"date\"])) & \n",
        "                                    (pd.to_datetime(earnings_df[\"date\"]) <= pd.to_datetime(end_date))]\n",
        "        return earnings_df\n",
        "\n",
        "\n",
        "\n",
        "    def json_to_dtf(self):\n",
        "        \"\"\" Parses through response and constructs and returns a dataframe\"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4q6TYiPP3kJF"
      },
      "outputs": [],
      "source": [
        "class ProcessData():\n",
        "    def join_all_datasets(self, arr, methods, on_columns):\n",
        "        \"\"\"\n",
        "        Joins all of the datasets in an array of dataframes (arr) using the\n",
        "        specified methods in the string array (methods). \n",
        "        Methods include full, left, right, inner\n",
        "        \n",
        "        Args:\n",
        "            arr (list): A list of pandas Dataframes to be joined.\n",
        "            methods (list): A list of strings representing join methods\n",
        "            on_columns (list): A list of specified column names to join on.\n",
        "            \n",
        "        Returns:\n",
        "            df_joined: The resulting DataFrame after all joins have been completed.\n",
        "        \n",
        "        Raises:\n",
        "            ValueError: If number of Dataframes passed in is less than 2\n",
        "            ValueError: If number of join methods passed in is not len(arr)-1\n",
        "            \"\"\"\n",
        "        \n",
        "        if len(arr) < 2:\n",
        "            raise ValueError(\"Number of dataframes must be at least 2 to perform a join\")\n",
        "        if  len(methods) != len(arr) -1:\n",
        "            raise ValueError(\"Number of join methods must be one less than number of dataframes to join\")\n",
        "        \n",
        "        df_joined = arr[0].copy()\n",
        "        for i in range(len(methods)):\n",
        "            How = methods[i]\n",
        "            df_to_join= arr[i+1]\n",
        "            On = on_columns[i]\n",
        "            df_joined = pd.merge(df_joined, df_to_join, how = How, on = On)\n",
        "        return df_joined\n",
        "\n",
        "    \"\"\" handle missing NaN values using an approach passed in as a string variable named approach for dataset d. Approaches allowed include one-hot encoding, dropping all rows with NaN, etc.\"\"\"\n",
        "\n",
        "    def handle_missing_values(self, approach, dataset):\n",
        "        pass\n",
        "    \"\"\" Normalize features of dataframe x using sklearn methods using an approach passed in as a string variable approach\"\"\"\n",
        "\n",
        "    def normalize_features(self, x, approach):\n",
        "        pass\n",
        "\n",
        "    \"\"\" Select target features of x, where the features are given in an array feature list\"\"\"\n",
        "\n",
        "    def select_features(self, dataset, feature_list):\n",
        "        pass\n",
        "    \"\"\" Splits data into training, validation, and testing \"\"\"\n",
        "\n",
        "    def split_data(self, dataset, column, test_size, val_size):\n",
        "        pass\n",
        "    \"\"\" Deletes rows of data containing outliers above a certain threshold from dataset \"\"\"\n",
        "\n",
        "    def handle_outliers(self, dataset, threshold):\n",
        "        pass\n",
        "    \n",
        "    # assuming that the ThetaData terminal is running\n",
        "    def test_method(self):\n",
        "        test = GatherData()\n",
        "        print(\"Fundamental\")\n",
        "        print(test.get_fundamental_data('AAPL', 5))\n",
        "        print(\"Options\")\n",
        "        print(test.get_historical_data(['AAPL'], \"20190109\", \"20240109\"))\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def zst_to_dataframe(file_path, delimiter=','):\n",
        "    \"\"\"\n",
        "    Converts a Zstandard-compressed file (.zst) to a pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the .zst file.\n",
        "        delimiter (str): Delimiter used in the CSV file (default is ',').\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The resulting pandas DataFrame.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            # Create a Zstandard decompressor\n",
        "            dctx = zstd.ZstdDecompressor()\n",
        "\n",
        "            # Decompress the file using a streaming reader\n",
        "            with dctx.stream_reader(f) as reader:\n",
        "                # Wrap the decompressed data into a text wrapper\n",
        "                decompressed_data = io.TextIOWrapper(reader, encoding='utf-8')\n",
        "\n",
        "                # Read the decompressed data into a DataFrame\n",
        "                df = pd.read_csv(decompressed_data, delimiter=delimiter)\n",
        "\n",
        "        return df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(f\"File is empty or corrupt: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing .zst file: {e}\")\n",
        "\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyzing historical data\n",
        "def graph_data(df_all): \n",
        "    sns.heatmap(df_all.corr()) ## df_all is complete data\n",
        "    sns.pairplot(df_all)\n",
        "\n",
        "def get_percent_changes(df):\n",
        "    df = df.sort_values(by='date')\n",
        "    df['weekly_pct_change'] = np.nan\n",
        "    for i in range(1, len(df)):\n",
        "        current_date = df.iloc[i]['date'] ## change to date collumn name\n",
        "        current_price = df.iloc[i]['price'] ## change to price collumn name\n",
        "        week_ago_date = current_date - pd.Timedelta(days=7)\n",
        "        previous_data = df[df['date'] == week_ago_date]\n",
        "        if not previous_data.empty:\n",
        "            previous_price = previous_data.iloc[0]['price'] ## change to price collumn name\n",
        "            pct_change = ((current_price - previous_price) / previous_price) * 100\n",
        "            df.at[i, 'weekly_pct_change'] = pct_change\n",
        "        else:\n",
        "            df.at[i, 'weekly_pct_change'] = np.nan\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1myFYcBO2kng"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ticker        date  pos_em_count  neg_em_count  em_total_count  \\\n",
            "0  GOOGL  2024-03-16             5             2               9   \n",
            "1  GOOGL  2024-03-17             3             2               8   \n",
            "2  GOOGL  2024-03-18           110            71             308   \n",
            "3  GOOGL  2024-03-19            40            13              87   \n",
            "4  GOOGL  2024-03-20            18            14              54   \n",
            "\n",
            "   pos_nem_count  neu_em_count  neu_nem_count  neg_nem_count  nem_total_count  \\\n",
            "0             10             2              1              5               16   \n",
            "1              8             3              1              4               13   \n",
            "2            127           127             81             87              295   \n",
            "3             50            34             26             34              110   \n",
            "4             22            22             22             28               72   \n",
            "\n",
            "   pos_total_count  neu_total_count  neg_total_count  total_count  \\\n",
            "0               15                3                7           25   \n",
            "1               11                4                6           21   \n",
            "2              237              208              158          603   \n",
            "3               90               60               47          197   \n",
            "4               40               44               42          126   \n",
            "\n",
            "   article_sentiment  article_count  \n",
            "0                0.0            0.0  \n",
            "1                0.0            0.0  \n",
            "2                0.0            0.0  \n",
            "3                0.0            0.0  \n",
            "4                2.0            2.0  \n"
          ]
        }
      ],
      "source": [
        "# test code for fundamental data\n",
        "test = GatherData()\n",
        "# fundamental_data = test.get_fundamental_data('GOOGL', \"2024-01-10\", \"2025-01-10\")\n",
        "# historical_data = test.get_historical_data(['GOOGL'], \"2024-01-10\", \"2025-01-10\")\n",
        "# liquidity_data = test.get_liquidity_data(['GOOGL'],\"2024-01-10\",\"2025-01-10\")\n",
        "# institutional_data = test.get_institutional_data(\"institutional_data.csv\", [\"GOOGL\"], \"2022-01-10\", \"2025-01-10\")\n",
        "\n",
        "# get data for the past 5 years starting from 2025-01-10\n",
        "sentiment_data = test.get_sentiment_data([\"GOOGL\"], \"2020-01-01\", \"2025-01-10\")\n",
        "earnings_data = test.get_earnings_data([\"GOOGL\"], \"2020-01-01\", \"2025-01-10\")\n",
        "\n",
        "print(sentiment_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         date  estimated_eps  reported_eps ticker\n",
            "4  2024-10-28           1.85          2.12  GOOGL\n",
            "5  2024-07-22           1.84          1.89  GOOGL\n",
            "6  2024-04-24           1.51          1.89  GOOGL\n",
            "7  2024-01-29           1.59          1.64  GOOGL\n"
          ]
        }
      ],
      "source": [
        "# # print(fundamental_data)\n",
        "# for df in [fundamental_data, liquidity_data, institutional_data, sentiment_data, earnings_data]:\n",
        "#     print(df.dtypes)\n",
        "#     # print(df.head())\n",
        "#     print()\n",
        "# # fundamenta: camelcase to snake_case\n",
        "# # institutional_data: uppercase to snake_case\n",
        "\n",
        "# df = pd.merge(fundamental_data, liquidity_data, how=\"outer\", on=[\"ticker\", \"date\"])\n",
        "df = pd.merge(df, sentiment_data, how=\"outer\", on=[\"ticker\", \"date\"])\n",
        "df = pd.merge(df, earnings_data, how=\"outer\", on=[\"ticker\", \"date\"])\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>close</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>open</th>\n",
              "      <th>volume</th>\n",
              "      <th>adjClose</th>\n",
              "      <th>adjHigh</th>\n",
              "      <th>adjLow</th>\n",
              "      <th>adjOpen</th>\n",
              "      <th>adjVolume</th>\n",
              "      <th>divCash</th>\n",
              "      <th>splitFactor</th>\n",
              "      <th>ticker</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-01-02</td>\n",
              "      <td>1368.68</td>\n",
              "      <td>1368.680</td>\n",
              "      <td>1346.490</td>\n",
              "      <td>1348.410</td>\n",
              "      <td>1364265</td>\n",
              "      <td>68.186312</td>\n",
              "      <td>68.186312</td>\n",
              "      <td>67.080828</td>\n",
              "      <td>67.176481</td>\n",
              "      <td>27285300</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>GOOGL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-01-03</td>\n",
              "      <td>1361.52</td>\n",
              "      <td>1373.750</td>\n",
              "      <td>1347.320</td>\n",
              "      <td>1348.000</td>\n",
              "      <td>1170629</td>\n",
              "      <td>67.829608</td>\n",
              "      <td>68.438895</td>\n",
              "      <td>67.122178</td>\n",
              "      <td>67.156055</td>\n",
              "      <td>23412580</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>GOOGL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-01-06</td>\n",
              "      <td>1397.81</td>\n",
              "      <td>1398.320</td>\n",
              "      <td>1351.000</td>\n",
              "      <td>1351.630</td>\n",
              "      <td>2339343</td>\n",
              "      <td>69.637541</td>\n",
              "      <td>69.662948</td>\n",
              "      <td>67.305512</td>\n",
              "      <td>67.336898</td>\n",
              "      <td>46786860</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>GOOGL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-01-07</td>\n",
              "      <td>1395.11</td>\n",
              "      <td>1403.500</td>\n",
              "      <td>1391.560</td>\n",
              "      <td>1400.460</td>\n",
              "      <td>1726456</td>\n",
              "      <td>69.503029</td>\n",
              "      <td>69.921011</td>\n",
              "      <td>69.326172</td>\n",
              "      <td>69.769561</td>\n",
              "      <td>34529120</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>GOOGL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-01-08</td>\n",
              "      <td>1405.04</td>\n",
              "      <td>1411.850</td>\n",
              "      <td>1392.630</td>\n",
              "      <td>1394.820</td>\n",
              "      <td>1766274</td>\n",
              "      <td>69.997732</td>\n",
              "      <td>70.337000</td>\n",
              "      <td>69.379478</td>\n",
              "      <td>69.488582</td>\n",
              "      <td>35325480</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>GOOGL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1253</th>\n",
              "      <td>2024-12-24</td>\n",
              "      <td>196.11</td>\n",
              "      <td>196.110</td>\n",
              "      <td>193.780</td>\n",
              "      <td>194.840</td>\n",
              "      <td>10403259</td>\n",
              "      <td>196.110000</td>\n",
              "      <td>196.110000</td>\n",
              "      <td>193.780000</td>\n",
              "      <td>194.840000</td>\n",
              "      <td>10403259</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>GOOGL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1254</th>\n",
              "      <td>2024-12-26</td>\n",
              "      <td>195.60</td>\n",
              "      <td>196.748</td>\n",
              "      <td>194.375</td>\n",
              "      <td>195.150</td>\n",
              "      <td>12057210</td>\n",
              "      <td>195.600000</td>\n",
              "      <td>196.748000</td>\n",
              "      <td>194.375000</td>\n",
              "      <td>195.150000</td>\n",
              "      <td>12057210</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>GOOGL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1255</th>\n",
              "      <td>2024-12-27</td>\n",
              "      <td>192.76</td>\n",
              "      <td>195.320</td>\n",
              "      <td>190.650</td>\n",
              "      <td>194.950</td>\n",
              "      <td>18891362</td>\n",
              "      <td>192.760000</td>\n",
              "      <td>195.320000</td>\n",
              "      <td>190.650000</td>\n",
              "      <td>194.950000</td>\n",
              "      <td>18891362</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>GOOGL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1256</th>\n",
              "      <td>2024-12-30</td>\n",
              "      <td>191.24</td>\n",
              "      <td>192.550</td>\n",
              "      <td>189.120</td>\n",
              "      <td>189.800</td>\n",
              "      <td>14264659</td>\n",
              "      <td>191.240000</td>\n",
              "      <td>192.550000</td>\n",
              "      <td>189.120000</td>\n",
              "      <td>189.800000</td>\n",
              "      <td>14264659</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>GOOGL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1257</th>\n",
              "      <td>2024-12-31</td>\n",
              "      <td>189.30</td>\n",
              "      <td>191.960</td>\n",
              "      <td>188.510</td>\n",
              "      <td>191.075</td>\n",
              "      <td>17466919</td>\n",
              "      <td>189.300000</td>\n",
              "      <td>191.960000</td>\n",
              "      <td>188.510000</td>\n",
              "      <td>191.075000</td>\n",
              "      <td>17466919</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>GOOGL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1258 rows  14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            date    close      high       low      open    volume    adjClose  \\\n",
              "0     2020-01-02  1368.68  1368.680  1346.490  1348.410   1364265   68.186312   \n",
              "1     2020-01-03  1361.52  1373.750  1347.320  1348.000   1170629   67.829608   \n",
              "2     2020-01-06  1397.81  1398.320  1351.000  1351.630   2339343   69.637541   \n",
              "3     2020-01-07  1395.11  1403.500  1391.560  1400.460   1726456   69.503029   \n",
              "4     2020-01-08  1405.04  1411.850  1392.630  1394.820   1766274   69.997732   \n",
              "...          ...      ...       ...       ...       ...       ...         ...   \n",
              "1253  2024-12-24   196.11   196.110   193.780   194.840  10403259  196.110000   \n",
              "1254  2024-12-26   195.60   196.748   194.375   195.150  12057210  195.600000   \n",
              "1255  2024-12-27   192.76   195.320   190.650   194.950  18891362  192.760000   \n",
              "1256  2024-12-30   191.24   192.550   189.120   189.800  14264659  191.240000   \n",
              "1257  2024-12-31   189.30   191.960   188.510   191.075  17466919  189.300000   \n",
              "\n",
              "         adjHigh      adjLow     adjOpen  adjVolume  divCash  splitFactor  \\\n",
              "0      68.186312   67.080828   67.176481   27285300      0.0          1.0   \n",
              "1      68.438895   67.122178   67.156055   23412580      0.0          1.0   \n",
              "2      69.662948   67.305512   67.336898   46786860      0.0          1.0   \n",
              "3      69.921011   69.326172   69.769561   34529120      0.0          1.0   \n",
              "4      70.337000   69.379478   69.488582   35325480      0.0          1.0   \n",
              "...          ...         ...         ...        ...      ...          ...   \n",
              "1253  196.110000  193.780000  194.840000   10403259      0.0          1.0   \n",
              "1254  196.748000  194.375000  195.150000   12057210      0.0          1.0   \n",
              "1255  195.320000  190.650000  194.950000   18891362      0.0          1.0   \n",
              "1256  192.550000  189.120000  189.800000   14264659      0.0          1.0   \n",
              "1257  191.960000  188.510000  191.075000   17466919      0.0          1.0   \n",
              "\n",
              "     ticker  \n",
              "0     GOOGL  \n",
              "1     GOOGL  \n",
              "2     GOOGL  \n",
              "3     GOOGL  \n",
              "4     GOOGL  \n",
              "...     ...  \n",
              "1253  GOOGL  \n",
              "1254  GOOGL  \n",
              "1255  GOOGL  \n",
              "1256  GOOGL  \n",
              "1257  GOOGL  \n",
              "\n",
              "[1258 rows x 14 columns]"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fundamental_data = GatherData()\n",
        "fundamental_data.get_fundamental_data(\"GOOGL\", start_date=\"2020-01-01\", end_date=\"2025-01-01\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
