{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/themoonwalker1/quantcap-options/blob/main/options_data_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "o7Z9N3kV2llC"
      },
      "outputs": [],
      "source": [
        "# Quant Cap Options Trading 23/24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta, date\n",
        "import json\n",
        "import httpx\n",
        "import csv\n",
        "import zstandard as zstd\n",
        "import io\n",
        "import yfinance as yf\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from httpx import HTTPStatusError\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os.path as osp\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "UQ7MkZuo3eN3"
      },
      "outputs": [],
      "source": [
        "class GatherData():\n",
        "    \n",
        "    def get_historical_data(self, symbols: list[str], start_date: str, end_date: str, save_to_csv: bool = True) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "       \n",
        "        Pulls EOD Options, EOD Greeks, and Open Interest data from ThetaData API and returns it as a DataFrame\n",
        "         \n",
        "        Retrieves data pertaining to the given parameters (params) from the\n",
        "        historical option and Greeks EOD and Open Interest endpoints. The data\n",
        "        includes all possible expiration dates and strike prices within the\n",
        "        specified range. Reference: https://http-docs.thetadata.us/\n",
        "\n",
        "\n",
        "        Args:\n",
        "            symbols: A list of strings representing the tickers to fetch historical option data.\n",
        "            start_date: A string representing the start date for historical option data\n",
        "            end_date: A string representing the end date for the historical option data\n",
        "            save_to_csv: A boolean for whether to save the dataframe as a csv file\n",
        "\n",
        "        Returns:\n",
        "            merged_df: A combined Pandas DataFrame of historical option EOD, historical Greeks EOD, and Open Interest data\n",
        "\n",
        "        Raises:\n",
        "            HTTPError: If the API request fails.\n",
        "            ValueError: If the response data is not in the expected format.\n",
        "        \"\"\"\n",
        "       \n",
        "        # BASE_URL = \"http://127.0.0.1:25510/v2\"\n",
        "        BASE_URL = \"http://127.0.0.1:25512/v2\"\n",
        "\n",
        "        tot_data = []\n",
        "        oi_data = []\n",
        "\n",
        "        start_date_parsed = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
        "        end_date_parsed = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
        "\n",
        "        header_eod = None\n",
        "        header_oi = None\n",
        "        for symbol in symbols:\n",
        "            current_date = start_date_parsed\n",
        "            while current_date <= end_date_parsed:              \n",
        "                params = {\n",
        "                    \"root\": symbol,\n",
        "                    \"start_date\": current_date.strftime(\"%Y%m%d\"),\n",
        "                    \"end_date\": current_date.strftime(\"%Y%m%d\"),\n",
        "                    \"exp\": 0,\n",
        "                    \"use_csv\": 'true'\n",
        "                }\n",
        "\n",
        "                urleod = BASE_URL + '/bulk_hist/option/eod_greeks'\n",
        "\n",
        "                header = None\n",
        "                while urleod is not None:\n",
        "                    try:\n",
        "                        response = httpx.get(urleod, params=params)\n",
        "                        response.raise_for_status()\n",
        "                        csv_data = csv.reader(response.text.split(\"\\n\"))\n",
        "                        if header is None:\n",
        "                            header = next(csv_data)\n",
        "                            if header_eod is None:\n",
        "                                header_eod = header\n",
        "                                expiration_idx_eod = header_eod.index('expiration')\n",
        "                                tot_data.append(header_eod)\n",
        "               \n",
        "                        for line in csv_data:\n",
        "                            if line:\n",
        "                                if True:\n",
        "                                    expiration_date = datetime.strptime(line[expiration_idx_eod], '%Y%m%d')\n",
        "                                    date_diff = (expiration_date - current_date).days\n",
        "                                    if 0 <= date_diff <= 30:\n",
        "                                        tot_data.append(line)\n",
        "                               \n",
        "                        if 'Next-Page' in response.headers and response.headers['Next-Page'] != \"null\":\n",
        "                            urleod = response.headers['Next-Page']\n",
        "                        else:\n",
        "                            urleod = None\n",
        "                    except HTTPStatusError as e:\n",
        "                        if e.response.status_code == 472:\n",
        "                            print(f\"No results for {symbol} on {current_date.strftime('%Y-%m-%d')} for EOD Greeks, skipping.\")\n",
        "                            break\n",
        "                        else:\n",
        "                            raise\n",
        "\n",
        "                urloi = BASE_URL + '/bulk_hist/option/open_interest'\n",
        "\n",
        "                header = None\n",
        "                while urloi is not None:\n",
        "                    try:\n",
        "                        response = httpx.get(urloi, params=params)\n",
        "                        response.raise_for_status()\n",
        "                        csv_data = csv.reader(response.text.split(\"\\n\"))\n",
        "                        if header is None:\n",
        "                            header = next(csv_data)\n",
        "                            if header_oi is None:\n",
        "                                header_oi = header\n",
        "                                expiration_idx_oi = header_oi.index('expiration')\n",
        "                                oi_data.append(header_oi)\n",
        "                        for line in csv_data:\n",
        "                            if line:\n",
        "                                if True:\n",
        "                                    expiration_date = datetime.strptime(line[expiration_idx_oi], '%Y%m%d')\n",
        "                                    date_diff = (expiration_date - current_date).days\n",
        "                                    if 0 <= date_diff <= 30:\n",
        "                                        oi_data.append(line)\n",
        "                                       \n",
        "                        if 'Next-Page' in response.headers and response.headers['Next-Page'] != \"null\":\n",
        "                            urloi = response.headers['Next-Page']\n",
        "                        else:\n",
        "                            urloi = None\n",
        "                    except HTTPStatusError as e:\n",
        "                        if e.response.status_code == 472:\n",
        "                            print(f\"No results for {symbol} on {current_date.strftime('%Y-%m-%d')} for Open Interest, skipping.\")\n",
        "                            break\n",
        "                        else:\n",
        "                            raise  \n",
        "                current_date += timedelta(days=1)\n",
        "\n",
        "        def process(data):\n",
        "            df = pd.DataFrame(data)\n",
        "            df.columns = df.iloc[0]\n",
        "            df = df.drop(0).reset_index(drop=True)\n",
        "            df['expiration'] = pd.to_datetime(df['expiration'], format='%Y%m%d', errors='coerce')\n",
        "            if df['expiration'].isnull().any():\n",
        "                print(\"Warning: Some 'expiration' values could not be parsed and were set to NaT.\")\n",
        "            df['date'] = pd.to_datetime(df['date'], format='%Y%m%d', errors='coerce')\n",
        "            if df['date'].isnull().any():\n",
        "                print(\"Warning: Some 'date' values could not be parsed and were set to NaT.\")\n",
        "            return df\n",
        "        df_eod = process(tot_data)\n",
        "        df_oi = process(oi_data)\n",
        "\n",
        "        df_merged = pd.merge(df_eod, df_oi, on=['root', 'expiration', 'strike', 'right', 'date'], how='left')\n",
        "\n",
        "        df_merged = df_merged[['date', 'root', 'expiration', 'strike', 'right', 'close', 'volume', 'count', \n",
        "                               'bid', 'bid_size', 'ask', 'ask_size', 'open_interest', 'delta', 'theta', 'vega', 'rho', \n",
        "                               'epsilon', 'lambda', 'gamma', 'd1', 'd2', 'implied_vol', 'iv_error', 'underlying_price']]\n",
        "\n",
        "        if save_to_csv:\n",
        "            df_merged.to_csv(f'historical_data_{\"_\".join(symbols)}_{start_date}-{end_date}.csv', index=False)\n",
        "\n",
        "        return df_merged\n",
        "\n",
        "\n",
        "\n",
        "    def get_sentiment_data(self, symbols: list[str], start_date: str, end_date: str, interval: str = \"1d\") -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Fetch sentiment data from StockGeist API and return it as a DataFrame.\n",
        "\n",
        "        This function retrieves sentiment data for a specified asset class and\n",
        "        location from the StockGeist API. The data includes both message and\n",
        "        article sentiment metrics for a given symbol over a specified date range.\n",
        "        Reference: https://docs.stockgeist.ai/\n",
        "\n",
        "        Args:\n",
        "            symbols: A list of strings representing the tickers to fetch sentiment data for.\n",
        "                eg: [\"AAPL\", \"MSFT\", \"GOOGL\"]\n",
        "            start_date: A string representing the start date for the sentiment data.\n",
        "            end_date: A string representing the end date for the sentiment data.\n",
        "            interval: A string representing the interval for the sentiment data.\n",
        "        Returns:\n",
        "            DataFrame: A pandas DataFrame containing the sentiment data.\n",
        "\n",
        "        Raises:\n",
        "            HTTPError: If the API request fails.\n",
        "            ValueError: If the response data is not in the expected format.\n",
        "        \"\"\"\n",
        "\n",
        "        # constants\n",
        "        EM_WEIGHT = 1.5 # weight of emotional sentiment\n",
        "        NEM_WEIGHT = 1.0 # weight of non-emotional sentiment\n",
        "\n",
        "        MESSAGE_WEIGHT = 0.3\n",
        "        ARTICLE_WEIGHT = 0.7\n",
        "\n",
        "        # API Key\n",
        "        STOCKGEIST_API_KEY = 'oElOiXrwLtDY2flizauAIrrpivXU0bUQ'\n",
        "        headers = {\"token\": STOCKGEIST_API_KEY}\n",
        "\n",
        "        # API endpoint\n",
        "        base_url = \"https://api.stockgeist.ai\"\n",
        "        asset_class = \"stock\"  # or \"crypto\"\n",
        "        location = \"us\"  # or \"global\"\n",
        "\n",
        "        # Messages sentiment data\n",
        "        messages_url = f\"{base_url}/{asset_class}/{location}/hist/message-metrics\"\n",
        "        messages_params = {\n",
        "            \"symbols\": ','.join(symbols),\n",
        "            \"start\": start_date,\n",
        "            \"end\": end_date,\n",
        "            \"timeframe\": interval\n",
        "        }\n",
        "        \n",
        "        # response = requests.get(messages_url, headers=headers, params=messages_params)\n",
        "        # if response.status_code != 200:\n",
        "        #     raise requests.HTTPError(f\"{response.status_code}: {response.text}\")\n",
        "        \n",
        "        # messages_data = response.json().get(\"data\", {})\n",
        "        messages_data = json.load(open(\"messages.json\")).get(\"data\", {})\n",
        "\n",
        "        messages_date_rows = []\n",
        "        for symbol in symbols: # same stocks in symbols\n",
        "            stock_data = messages_data.get(symbol, [])\n",
        "            for day_data in stock_data:\n",
        "                date = datetime.fromisoformat(day_data.get(\"timestamp\")).date() # date only\n",
        "                pos_em_count = day_data.get(\"pos_em_count\", 0)\n",
        "                pos_nem_count = day_data.get(\"pos_nem_count\", 0)\n",
        "                neu_em_count = day_data.get(\"neu_em_count\", 0)\n",
        "                neu_nem_count = day_data.get(\"neu_nem_count\", 0)\n",
        "                neg_em_count = day_data.get(\"neg_em_count\", 0)\n",
        "                neg_nem_count = day_data.get(\"neg_nem_count\", 0)\n",
        "                em_total_count = day_data.get(\"em_total_count\", 0)\n",
        "                nem_total_count = day_data.get(\"nem_total_count\", 0)\n",
        "                pos_total_count = day_data.get(\"pos_total_count\", 0)\n",
        "                neu_total_count = day_data.get(\"neu_total_count\", 0)\n",
        "                neg_total_count = day_data.get(\"neg_total_count\", 0)\n",
        "                total_count = day_data.get(\"total_count\", 0)\n",
        "\n",
        "                # calculate message sentiment using custom formula\n",
        "                #normalized: 0 to 1\n",
        "                # message_sentiment = EM_WEIGHT * (pos_em_count + neg_em_count) / em_total_count + NEM_WEIGHT * (pos_nem_count + neg_nem_count) / nem_total_count\n",
        "                # messages_date_rows.append([symbol, date, message_sentiment])\n",
        "                messages_date_rows.append([symbol, date, pos_em_count, neg_em_count, em_total_count, pos_nem_count, neu_em_count, neu_nem_count, neg_nem_count, nem_total_count, pos_total_count, neu_total_count, neg_total_count, total_count])\n",
        "\n",
        "        messages_df = pd.DataFrame(messages_date_rows, columns=[\"ticker\", \"date\", \"pos_em_count\", \"neg_em_count\", \"em_total_count\", \"pos_nem_count\", \"neu_em_count\", \"neu_nem_count\", \"neg_nem_count\", \"nem_total_count\", \"pos_total_count\", \"neu_total_count\", \"neg_total_count\", \"total_count\"])\n",
        "\n",
        "        # Articles sentiment data\n",
        "        articles_url = f\"{base_url}/{asset_class}/{location}/hist/article-metrics\"\n",
        "        articles_params = {\n",
        "            \"symbols\": ','.join(symbols),\n",
        "            \"start\": start_date,\n",
        "            \"end\": end_date,\n",
        "            \"timeframe\": interval,\n",
        "            \"max_symbol_articles\": 200,\n",
        "            \"sort_by\": \"timestamp\"\n",
        "        }\n",
        "        # response = requests.get(articles_url, headers=headers, params=articles_params)\n",
        "        # if response.status_code != 200:\n",
        "        #     raise requests.HTTPError(f\"{response.status_code}: {response.text}\")\n",
        "        \n",
        "        # articles_data = response.json().get(\"data\", {})\n",
        "\n",
        "        articles_data = json.load(open(\"articles.json\")).get(\"data\", {})\n",
        "\n",
        "        articles_date_rows = []\n",
        "        for symbol in symbols:\n",
        "            stock_data = articles_data.get(symbol, [])  \n",
        "            for day_data in stock_data:\n",
        "                date = datetime.fromisoformat(day_data.get(\"timestamp\")).date() # date only\n",
        "                mentions = day_data.get(\"mentions\", 0)\n",
        "                title_sentiment = day_data.get(\"title_sentiment\", \"neutral\")\n",
        "                sentiment_map = {\"neutral\": 0, \"positive\": 1, \"negative\": -1} # # neutral, positive, negative\n",
        "                title_sentiment = sentiment_map.get(title_sentiment, 0)\n",
        "                # ignore title, summary, original_url, img_url, sentiment_spans\n",
        "\n",
        "                # calculate article sentiment using custom formula\n",
        "                article_sentiment = title_sentiment * mentions  \n",
        "\n",
        "                articles_date_rows.append([symbol, date, article_sentiment, mentions])\n",
        "\n",
        "        # symbol/date pair should be unique\n",
        "        articles_df = pd.DataFrame(articles_date_rows, columns=[\"ticker\", \"date\", \"article_sentiment\", \"article_count\"]).groupby(['ticker', 'date'], as_index=False)[['article_sentiment', 'article_count']].sum()\n",
        "\n",
        "\n",
        "        # combine messages and articles dataframes\n",
        "        sentiment_df = pd.merge(messages_df, articles_df, on=[\"ticker\", \"date\"], how=\"outer\")\n",
        "        sentiment_df.fillna(0, inplace=True)\n",
        "        # sentiment_df.set_index([\"symbol\", \"date\"], inplace=True)\n",
        "\n",
        "        # calculate weighted sentiment\n",
        "        # sentiment_df[\"weighted_sentiment\"] = MESSAGE_WEIGHT * sentiment_df[\"message_sentiment\"] + ARTICLE_WEIGHT * sentiment_df[\"article_sentiment\"]\n",
        "\n",
        "        return sentiment_df\n",
        "\n",
        "\n",
        "    def get_fundamental_data(self, ticker: str, total_years: int):\n",
        "        \"\"\" \n",
        "        Pulls historical stock data from Tiingo using its API and \n",
        "        preprocesses it by getting the percentage stock increase per \n",
        "        week from the raw EOD close data\n",
        "\n",
        "        Args: \n",
        "            ticker: A string with the specified stock ticker\n",
        "            total_years: How many years back to get the data from the current date. \n",
        "            Must be less than 30. \n",
        "        Returns: \n",
        "            Pandas Dataframe with two columns: the date and the percentage stock increase a week out into the future. \n",
        "            The date is an integer in the form YYYY-MM-DD, and the percentage stock increase is a float. \n",
        "        Raises: \n",
        "            ValueError if the total_years inputted are greater than 30. \n",
        "        \"\"\"\n",
        "        if total_years > 30:\n",
        "            raise ValueError(\"Total years are greater than 30\")\n",
        "        file_name = \"fundamental\"\n",
        "        cur_path = osp.dirname(osp.abspath(\"__file__\"))\n",
        "        path_to_file = osp.join(cur_path, '{}.pickle'.format(file_name))\n",
        "        to_return = None\n",
        "        # if osp.isfile(path_to_file):\n",
        "        #     to_return = pickle.load(open(path_to_file, 'rb'))[0]\n",
        "        # else:\n",
        "        TIINGO_API_KEY = '95b8e93dadad1cceda98479bc2420f9a0bb5556a'\n",
        "        base_url = \"https://api.tiingo.com/tiingo/daily\"\n",
        "        curr_date = date.today()\n",
        "        start_date = curr_date + relativedelta(years=total_years)\n",
        "        str_end_date = str(curr_date)\n",
        "        str_start_date = str(start_date)\n",
        "        url = f\"{base_url}/{ticker.lower()}/prices?startDate={str_start_date}&endDate={str_end_date}&token={TIINGO_API_KEY}\"\n",
        "        headers = {\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "        requestResponse = requests.get(url, headers=headers)\n",
        "        requestResponse = requestResponse.json()\n",
        "        size = len(requestResponse)\n",
        "        to_return = {\"date\": [None] * size, \"pct_change\": np.ndarray((size))}\n",
        "        for i in range(7, len(requestResponse)):\n",
        "            entry = requestResponse[i]\n",
        "            temp_date = entry[\"date\"]\n",
        "            end_date = datetime.fromisoformat(temp_date[:-1] + '+00:00') - relativedelta(days=7)\n",
        "            end_date = end_date.isoformat()[:-6] + '.000Z'\n",
        "            match_close = [a['close'] for a in requestResponse if a['date']== end_date]\n",
        "            if len(match_close) != 0:\n",
        "                day = temp_date[:10]\n",
        "                close_1 = entry['close']\n",
        "                close_2 = match_close[0]\n",
        "                pct_change = (close_1 - close_2) / close_2 * 100\n",
        "                to_return[\"date\"][i] = day\n",
        "                to_return[\"pct_change\"][i] = pct_change\n",
        "            indices_of_none_dates = [x for x in range(len(to_return[\"date\"])) if to_return[\"date\"][x] is not None]\n",
        "            to_return[\"date\"] = [to_return[\"date\"][x] for x in indices_of_none_dates]\n",
        "            to_return[\"pct_change\"] = [to_return[\"pct_change\"][x] for x in indices_of_none_dates]\n",
        "        to_return = pd.DataFrame.from_dict(to_return)\n",
        "        to_return[\"ticker\"] = ticker\n",
        "            # with open(path_to_file, 'wb') as cachedfile:\n",
        "            #     pickle.dump((to_return,), cachedfile)\n",
        "        return to_return\n",
        "\n",
        "\n",
        "    def zst_to_dataframe(self, file_path, delimiter=','):\n",
        "        \"\"\"\n",
        "        Converts a Zstandard-compressed file (.zst) to a pandas DataFrame.\n",
        "\n",
        "        Args:\n",
        "            file_path (str): Path to the .zst file.\n",
        "            delimiter (str): Delimiter used in the CSV file (default is ',').\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: The resulting pandas DataFrame.\n",
        "        \"\"\"\n",
        "        with open(file_path, 'rb') as f:\n",
        "                # Create a Zstandard decompressor\n",
        "                dctx = zstd.ZstdDecompressor()\n",
        "\n",
        "                # Decompress the file using a streaming reader\n",
        "                with dctx.stream_reader(f) as reader:\n",
        "                    # Wrap the decompressed data into a text wrapper\n",
        "                    decompressed_data = io.TextIOWrapper(reader, encoding='utf-8')\n",
        "                    \n",
        "                    # Read the decompressed data into a DataFrame\n",
        "                    df = pd.read_csv(decompressed_data, delimiter=delimiter)\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def shrink_dataframe(df, fields_to_keep):\n",
        "        \"\"\"\n",
        "        Reduces a DataFrame to only the specified fields (columns).\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): The original DataFrame.\n",
        "            fields_to_keep (list): List of column names to retain.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: A reduced DataFrame containing only the specified fields.\n",
        "        \"\"\"\n",
        "        # Check if all fields to keep are in the DataFrame\n",
        "        missing_fields = [field for field in fields_to_keep if field not in df.columns]\n",
        "        if missing_fields:\n",
        "            raise ValueError(f\"The following fields are not in the DataFrame: {missing_fields}\")\n",
        "\n",
        "        # Keep only the specified columns\n",
        "        reduced_df = df[fields_to_keep]\n",
        "        return reduced_df\n",
        "    \n",
        "    def get_liquidity_data(self, symbols: list[str], start_date: str, end_date: str):\n",
        "        \"\"\"\n",
        "        Pulls data from NASDAQ, filters it by symbols and date range, and returns a DataFrame.\n",
        "\n",
        "        Args:\n",
        "            symbols: A list of strings representing the tickers to fetch liquidity data for.\n",
        "            start_date: A string representing the start date (inclusive) in \"YYYY-MM-DD\" format.\n",
        "            end_date: A string representing the end date (inclusive) in \"YYYY-MM-DD\" format.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame: A pandas DataFrame containing the filtered liquidity data.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If required fields are missing in the dataset.\n",
        "        \"\"\"\n",
        "        # File path for the data (replace with your actual .zst file path)\n",
        "        file_path = 'xnas-itch-20241209.mbp-1.csv.zst'\n",
        "        df = self.zst_to_dataframe(file_path)  # Load the .zst file into a DataFrame\n",
        "\n",
        "\n",
        "        # Fields to keep\n",
        "        fields_to_keep = [\n",
        "            \"ts_event\", \"action\", \"size\", \"price\", \"bid_px_00\", \"ask_px_00\", \n",
        "            \"bid_sz_00\", \"ask_sz_00\", \"bid_ct_00\", \"ask_ct_00\", \"symbol\"\n",
        "        ]\n",
        "        # Shrink the DataFrame to only the fields we need\n",
        "        df = df[fields_to_keep]\n",
        "\n",
        "        # Convert 'ts_event' to datetime and create a 'date' column\n",
        "        df[\"date\"] = pd.to_datetime(df[\"ts_event\"]).dt.date\n",
        "\n",
        "        # Filter by date range\n",
        "        start_date = pd.to_datetime(start_date).date()\n",
        "        end_date = pd.to_datetime(end_date).date()\n",
        "        df = df[(df[\"date\"] >= start_date) & (df[\"date\"] <= end_date)]\n",
        "\n",
        "        # Filter by symbols\n",
        "        df = df[df[\"symbol\"].isin(symbols)]\n",
        "        \n",
        "        df = df[df[\"action\"].isin([\"A\", \"T\"])]\n",
        "\n",
        "        # Rename 'symbol' to 'ticker'\n",
        "        df.rename(columns={\"symbol\": \"ticker\"}, inplace=True)\n",
        "\n",
        "        # Drop the 'ts_event' column\n",
        "        df.drop(columns=[\"ts_event\"], inplace=True)\n",
        "\n",
        "        return df\n",
        "\n",
        "        # Original file was >7GB thus most processing was done beforehand by code in 13F_parser.ipynb. institutional_data.csv is already cleaned.\n",
        "\n",
        "    def get_institutional_data(self, input_path, symbols: list[str], start_date: str, end_date: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Reads institutional data from a CSV file, filters it by symbols and date range,\n",
        "        and returns a DataFrame.\n",
        "\n",
        "        Args:\n",
        "            input_path (str): Path to the input CSV file.\n",
        "            symbols (list[str]): List of tickers to filter the data for.\n",
        "            start_date (str): Start date (inclusive) in \"YYYY-MM-DD\" format.\n",
        "            end_date (str): End date (inclusive) in \"YYYY-MM-DD\" format.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: A DataFrame containing the filtered institutional data.\n",
        "        \"\"\"\n",
        "        # Read the input CSV file\n",
        "        df = pd.read_csv(input_path, dtype=str)\n",
        "\n",
        "        # Ensure the 'date' column is properly converted to datetime\n",
        "        if \"date\" not in df.columns:\n",
        "            raise ValueError(\"The input CSV is missing a 'date' column.\")\n",
        "        \n",
        "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "        df.dropna(subset=[\"date\"], inplace=True)  # Remove rows with invalid dates\n",
        "\n",
        "        # Filter by date range\n",
        "        start_date = pd.to_datetime(start_date)\n",
        "        end_date = pd.to_datetime(end_date)\n",
        "        df = df[(df[\"date\"] >= start_date) & (df[\"date\"] <= end_date)]\n",
        "\n",
        "        # Filter data by symbols\n",
        "        if \"Ticker\" not in df.columns:\n",
        "            raise ValueError(\"The input CSV is missing a 'Ticker' column.\")\n",
        "        \n",
        "        df = df[df[\"FILINGMANAGER_NAME\"].isin(symbols)]\n",
        "\n",
        "        # Return the filtered DataFrame\n",
        "        return df\n",
        "\n",
        "    def get_earnings_data(self, symbols: list[str], start_date: str, end_date: str):\n",
        "        \"\"\"\n",
        "        Fetch earnings data from Yahoo Calendar and return it as a DataFrame.\n",
        "\n",
        "        This function retrieves earnings data for a specified list of tickers\n",
        "        from the Yahoo Calendar API. The data includes earnings dates and other\n",
        "        relevant financial information for the given tickers over a specified date range.\n",
        "        Reference:  https://pypi.org/project/yfinance/\n",
        "                    https://ranaroussi.github.io/yfinance/index.html    \n",
        "\n",
        "        Args:\n",
        "            symbols: A list of strings representing the tickers to fetch earnings data for.\n",
        "                eg: [\"AAPL\", \"MSFT\", \"GOOGL\"]\n",
        "            start_date: A string representing the start date for the earnings data.\n",
        "            end_date: A string representing the end date for the earnings data.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame: A pandas DataFrame containing the earnings data.\n",
        "\n",
        "        Raises:\n",
        "            HTTPError: If the API request fails.\n",
        "            ValueError: If the response data is not in the expected format.\n",
        "        \"\"\"\n",
        "        \n",
        "        earnings_df = pd.DataFrame(columns=[\"symbol\", \"earnings_date\", \"estimated_eps\", \"reported_eps\"])\n",
        "        for symbol in symbols:\n",
        "            ticker = yf.Ticker(symbol)\n",
        "            earning_dates_df = ticker.earnings_dates\n",
        "            earning_dates_df.drop(columns=[\"Surprise(%)\"], inplace=True)\n",
        "            earning_dates_df.reset_index(drop=False, inplace=True)\n",
        "            earning_dates_df.columns= [\"earnings_date\", \"estimated_eps\", \"reported_eps\"]\n",
        "            earning_dates_df[\"symbol\"] = symbol\n",
        "            earning_dates_df[\"earnings_date\"] = earning_dates_df[\"earnings_date\"].dt.date\n",
        "            earnings_df = pd.concat([earnings_df, earning_dates_df], ignore_index=True)\n",
        "            earnings_df.dropna(axis=1, how='all', inplace=True)\n",
        "        return earnings_df\n",
        "\n",
        "\n",
        "\n",
        "    def json_to_dtf(self):\n",
        "        \"\"\" Parses through response and constructs and returns a dataframe\"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  action  size   price  bid_px_00  ask_px_00  bid_sz_00  ask_sz_00  bid_ct_00  \\\n",
            "0      A   300  174.85        NaN     174.85          0        300          0   \n",
            "1      T   290  174.85        NaN     174.85          0        300          0   \n",
            "3      T    10  174.85        NaN     174.85          0         10          0   \n",
            "4      T    90  174.88        NaN     174.88          0        300          0   \n",
            "6      A    10  160.00      160.0     174.88         10        210          1   \n",
            "\n",
            "   ask_ct_00 ticker        date  \n",
            "0          1  GOOGL  2024-12-09  \n",
            "1          1  GOOGL  2024-12-09  \n",
            "3          1  GOOGL  2024-12-09  \n",
            "4          1  GOOGL  2024-12-09  \n",
            "6          1  GOOGL  2024-12-09  \n",
            "  Ticker VALUE SSHPRNAMT SSHPRNAMTTYPE PUTCALL  \\\n",
            "0    AEF   237     26031            SH     NaN   \n",
            "1    ALB  4752     21703            SH     NaN   \n",
            "2   ABEV    14      5104            SH     NaN   \n",
            "3   AZUL  1330     65020            SH     NaN   \n",
            "4   BSBR   214     31916            SH     NaN   \n",
            "\n",
            "                   FILINGMANAGER_NAME       date  \n",
            "0  Provida Pension Fund Administrator 2021-11-15  \n",
            "1  Provida Pension Fund Administrator 2021-11-15  \n",
            "2  Provida Pension Fund Administrator 2021-11-15  \n",
            "3  Provida Pension Fund Administrator 2021-11-15  \n",
            "4  Provida Pension Fund Administrator 2021-11-15  \n"
          ]
        }
      ],
      "source": [
        "# Testing liquidity and institutional\n",
        "analyzer = GatherData()\n",
        "liquidity_df = analyzer.get_liquidity_data([\"AAPL\", \"GOOGL\"], \"2024-12-07\", \"2024-12-12\")\n",
        "print(liquidity_df.head())\n",
        "\n",
        "input_path = \"institutional_data.csv\"  # CSV file path in the same directory\n",
        "institutional_df = analyzer.get_institutional_data(input_path, [\"Provida Pension Fund Administrator\"], \"2021-11-14\", \"2021-11-16\")\n",
        "print(institutional_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "ename": "ConnectError",
          "evalue": "[Errno 61] Connection refused",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/httpx/_transports/default.py:101\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/httpcore/_sync/connection.py:78\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/httpcore/_sync/connection.py:124\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnect_tcp\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m--> 124\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/httpcore/_backends/sync.py:207\u001b[0m, in \u001b[0;36mSyncBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    202\u001b[0m exc_map: ExceptionMapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    203\u001b[0m     socket\u001b[38;5;241m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[1;32m    205\u001b[0m }\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    208\u001b[0m     sock \u001b[38;5;241m=\u001b[39m socket\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[1;32m    209\u001b[0m         address,\n\u001b[1;32m    210\u001b[0m         timeout,\n\u001b[1;32m    211\u001b[0m         source_address\u001b[38;5;241m=\u001b[39msource_address,\n\u001b[1;32m    212\u001b[0m     )\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/httpcore/_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[0;31mConnectError\u001b[0m: [Errno 61] Connection refused",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[34], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Testing historical option \u001b[39;00m\n\u001b[1;32m      2\u001b[0m my_data \u001b[38;5;241m=\u001b[39m GatherData()\n\u001b[0;32m----> 5\u001b[0m historical_df \u001b[38;5;241m=\u001b[39m \u001b[43mmy_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_historical_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGOOGL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2020-01-01\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2025-01-01\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(historical_df)\n",
            "Cell \u001b[0;32mIn[32], line 55\u001b[0m, in \u001b[0;36mGatherData.get_historical_data\u001b[0;34m(self, symbols, start_date, end_date, save_to_csv)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m urleod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mhttpx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murleod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m         response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     57\u001b[0m         csv_data \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(response\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/httpx/_api.py:195\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, headers, cookies, auth, proxy, follow_redirects, verify, timeout, trust_env)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(\n\u001b[1;32m    175\u001b[0m     url: URL \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m     trust_env: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    186\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[1;32m    187\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m    Sends a `GET` request.\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m    on this function, as `GET` requests should not include a request body.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/httpx/_api.py:109\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, params, content, data, files, json, headers, cookies, auth, proxy, timeout, follow_redirects, verify, trust_env)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03mSends an HTTP request.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Client(\n\u001b[1;32m    103\u001b[0m     cookies\u001b[38;5;241m=\u001b[39mcookies,\n\u001b[1;32m    104\u001b[0m     proxy\u001b[38;5;241m=\u001b[39mproxy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m     trust_env\u001b[38;5;241m=\u001b[39mtrust_env,\n\u001b[1;32m    108\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m client:\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/httpx/_client.py:825\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    810\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    812\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    813\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    814\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    823\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    824\u001b[0m )\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/httpx/_transports/default.py:249\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhttpcore\u001b[39;00m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m    250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    151\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/httpx/_transports/default.py:118\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    117\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
            "\u001b[0;31mConnectError\u001b[0m: [Errno 61] Connection refused"
          ]
        }
      ],
      "source": [
        "# Testing historical option \n",
        "my_data = GatherData()\n",
        "\n",
        "\n",
        "historical_df = my_data.get_historical_data([\"GOOGL\"], \"2020-01-01\", \"2025-01-01\")\n",
        "print(historical_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing\n",
        "\n",
        "analyzer = GatherData()\n",
        "sentiment_df = analyzer.get_sentiment_data([\"AAPL\", \"GOOGL\"], \"2024-01-01\", \"2024-01-02\", \"1d\")\n",
        "print(sentiment_df.head())\n",
        "earnings_df = analyzer.get_earnings_data([\"AAPL\", \"GOOGL\"], \"2024-01-01\", \"2024-01-02\")\n",
        "print(earnings_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "4q6TYiPP3kJF"
      },
      "outputs": [],
      "source": [
        "class ProcessData():\n",
        "    def join_all_datasets(self, arr, methods, on_columns):\n",
        "        \"\"\"\n",
        "        Joins all of the datasets in an array of dataframes (arr) using the\n",
        "        specified methods in the string array (methods). \n",
        "        Methods include full, left, right, inner\n",
        "        \n",
        "        Args:\n",
        "            arr (list): A list of pandas Dataframes to be joined.\n",
        "            methods (list): A list of strings representing join methods\n",
        "            on_columns (list): A list of specified column names to join on.\n",
        "            \n",
        "        Returns:\n",
        "            df_joined: The resulting DataFrame after all joins have been completed.\n",
        "        \n",
        "        Raises:\n",
        "            ValueError: If number of Dataframes passed in is less than 2\n",
        "            ValueError: If number of join methods passed in is not len(arr)-1\n",
        "            \"\"\"\n",
        "        \n",
        "        if len(arr) < 2:\n",
        "            raise ValueError(\"Number of dataframes must be at least 2 to perform a join\")\n",
        "        if  len(methods) != len(arr) -1:\n",
        "            raise ValueError(\"Number of join methods must be one less than number of dataframes to join\")\n",
        "        \n",
        "        df_joined = arr[0].copy()\n",
        "        for i in range(len(methods)):\n",
        "            How = methods[i]\n",
        "            df_to_join= arr[i+1]\n",
        "            On = on_columns[i]\n",
        "            df_joined = pd.merge(df_joined, df_to_join, how = How, on = On)\n",
        "        return df_joined\n",
        "\n",
        "    \"\"\" handle missing NaN values using an approach passed in as a string variable named approach for dataset d. Approaches allowed include one-hot encoding, dropping all rows with NaN, etc.\"\"\"\n",
        "\n",
        "    def handle_missing_values(self, approach, dataset):\n",
        "        pass\n",
        "    \"\"\" Normalize features of dataframe x using sklearn methods using an approach passed in as a string variable approach\"\"\"\n",
        "\n",
        "    def normalize_features(self, x, approach):\n",
        "        pass\n",
        "\n",
        "    \"\"\" Select target features of x, where the features are given in an array feature list\"\"\"\n",
        "\n",
        "    def select_features(self, dataset, feature_list):\n",
        "        pass\n",
        "    \"\"\" Splits data into training, validation, and testing \"\"\"\n",
        "\n",
        "    def split_data(self, dataset, column, test_size, val_size):\n",
        "        pass\n",
        "    \"\"\" Deletes rows of data containing outliers above a certain threshold from dataset \"\"\"\n",
        "\n",
        "    def handle_outliers(self, dataset, threshold):\n",
        "        pass\n",
        "    \n",
        "    # assuming that the ThetaData terminal is running\n",
        "    def test_method(self):\n",
        "        test = GatherData()\n",
        "        print(\"Fundamental\")\n",
        "        print(test.get_fundamental_data('AAPL', 5))\n",
        "        print(\"Options\")\n",
        "        print(test.get_historical_data(['AAPL'], \"20190109\", \"20240109\"))\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def zst_to_dataframe(file_path, delimiter=','):\n",
        "    \"\"\"\n",
        "    Converts a Zstandard-compressed file (.zst) to a pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the .zst file.\n",
        "        delimiter (str): Delimiter used in the CSV file (default is ',').\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The resulting pandas DataFrame.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            # Create a Zstandard decompressor\n",
        "            dctx = zstd.ZstdDecompressor()\n",
        "\n",
        "            # Decompress the file using a streaming reader\n",
        "            with dctx.stream_reader(f) as reader:\n",
        "                # Wrap the decompressed data into a text wrapper\n",
        "                decompressed_data = io.TextIOWrapper(reader, encoding='utf-8')\n",
        "\n",
        "                # Read the decompressed data into a DataFrame\n",
        "                df = pd.read_csv(decompressed_data, delimiter=delimiter)\n",
        "\n",
        "        return df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(f\"File is empty or corrupt: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing .zst file: {e}\")\n",
        "\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyzing historical data\n",
        "def graph_data(df_all): \n",
        "    sns.heatmap(df_all.corr()) ## df_all is complete data\n",
        "    sns.pairplot(df_all)\n",
        "\n",
        "def get_percent_changes(df):\n",
        "    df = df.sort_values(by='date')\n",
        "    df['weekly_pct_change'] = np.nan\n",
        "    for i in range(1, len(df)):\n",
        "        current_date = df.iloc[i]['date'] ## change to date collumn name\n",
        "        current_price = df.iloc[i]['price'] ## change to price collumn name\n",
        "        week_ago_date = current_date - pd.Timedelta(days=7)\n",
        "        previous_data = df[df['date'] == week_ago_date]\n",
        "        if not previous_data.empty:\n",
        "            previous_price = previous_data.iloc[0]['price'] ## change to price collumn name\n",
        "            pct_change = ((current_price - previous_price) / previous_price) * 100\n",
        "            df.at[i, 'weekly_pct_change'] = pct_change\n",
        "        else:\n",
        "            df.at[i, 'weekly_pct_change'] = np.nan\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1myFYcBO2kng"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/h4/v901gzsn6dqb4xb5ffzltz3r0000gp/T/ipykernel_6942/4015056712.py:506: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  earnings_df = pd.concat([earnings_df, earning_dates_df], ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "# test code for fundamental data\n",
        "test = GatherData()\n",
        "fundamental_data = test.get_fundamental_data('GOOGL', 1)\n",
        "# historical_data = test.get_historical_data(['GOOGL'], \"2024-01-10\", \"2025-01-10\")\n",
        "liquidity_data = test.get_liquidity_data(['GOOGL'],\"2024-01-10\",\"2025-01-10\")\n",
        "institutional_data = test.get_institutional_data(\"institutional_data.csv\", [\"GOOGL\"], \"2024-01-10\", \"2025-01-10\")\n",
        "sentiment_data = test.get_sentiment_data([\"GOOGL\"], \"2024-01-10\", \"2025-01-10\")\n",
        "earnings_data = test.get_earnings_data([\"GOOGL\"], \"2024-01-10\", \"2025-01-10\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Empty DataFrame\n",
            "Columns: [date, pct_change, ticker]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "print(fundamental_data)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "options",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
